{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5tyCsTK0w_MX"
      },
      "outputs": [],
      "source": [
        "server_root_path = \"/content/drive/MyDrive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUq5TfGU0LmB"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "def crop_and_pad(img1, size=224.0, max_=0):\n",
        "    # print('## File: img_utills.py | Function: crop_and_pad ##')\n",
        "    h = img1.shape[0]\n",
        "    w = img1.shape[1]\n",
        "\n",
        "    # Maintain the same aspect ratio and resize the image\n",
        "\n",
        "    if h>w:\n",
        "        ratio = float(size)/float(h)\n",
        "        new_h = int(size)\n",
        "        new_w = int(ratio*w)\n",
        "    else:\n",
        "        ratio = float(size)/float(w)\n",
        "        # print(ratio)\n",
        "        new_h = int(ratio*h)\n",
        "        new_w = int(size)\n",
        "\n",
        "    # print(img1.shape)\n",
        "    # print('newsize', new_h, new_w)\n",
        "    img_resized = cv2.resize(img1, (new_w, new_h))\n",
        "\n",
        "    pad_h = (size-new_h)\n",
        "    pad_h_start = int(pad_h//2)\n",
        "    pad_h_stop = int(pad_h - pad_h_start)\n",
        "    pad_w = (size-new_w)\n",
        "    pad_w_start = int(pad_w//2)\n",
        "    pad_w_stop = int(pad_w - pad_w_start)\n",
        "\n",
        "    img_cropped = cv2.copyMakeBorder(img_resized,pad_h_start,pad_h_stop,pad_w_start,pad_w_stop, cv2.BORDER_CONSTANT, value=int(max_))\n",
        "\n",
        "    return img_cropped\n",
        "\n",
        "def save_image(img_path_read, data_dir, category, cat_it, dataset_name, iteration_no, phase, resolution):\n",
        "    # print('## File: dataset_utils.py | Function: save_image ##')\n",
        "    image = io.imread(img_path_read)\n",
        "    crop = crop_and_pad(image, size=resolution)\n",
        "    img_name = os.path.join(data_dir, phase,  '_'.join(['category', category, 'category_number', str(cat_it), 'dataset', dataset_name, str(iteration_no)])) + '.png'\n",
        "    image =  image.astype('uint8')\n",
        "    io.imsave(img_name, crop)\n",
        "\n",
        "def save_data(server_root_path, dataset_dir, dataset_exp_name, images_folder_name, datasets, C, C_dash, train_val_split, resolution):\n",
        "    # print('## File: dataset_utils.py | Function: save_data ##')\n",
        "    data_dir = os.path.join(server_root_path, dataset_dir, dataset_exp_name, images_folder_name)\n",
        "    print(data_dir)\n",
        "    # print data_dir\n",
        "    if os.path.exists(data_dir):\n",
        "        os.system('rm -rf ' + data_dir + '/*')\n",
        "    else:\n",
        "        os.mkdir(data_dir)\n",
        "\n",
        "    categories = list(C)\n",
        "    categories.extend(C_dash)\n",
        "\n",
        "    os.mkdir(os.path.join(data_dir, 'train'))\n",
        "    os.mkdir(os.path.join(data_dir, 'val'))\n",
        "    train_iteration_no = 0\n",
        "    val_iteration_no = 0\n",
        "\n",
        "    for cat_it, category in tqdm(list(enumerate(categories))):\n",
        "\n",
        "        for dataset_name in datasets:\n",
        "\n",
        "            imgs_path = np.array(glob.glob(os.path.join(server_root_path, dataset_dir, dataset_name, category) + '/*'))\n",
        "            np.random.shuffle(imgs_path)\n",
        "            split_pos = int(train_val_split*len(imgs_path))\n",
        "\n",
        "            imgs_path_train = imgs_path[:split_pos]\n",
        "            imgs_path_val = imgs_path[split_pos:]\n",
        "\n",
        "            #Save train images\n",
        "            for img_path_read in imgs_path_train:\n",
        "                # print(img_path_read)\n",
        "                save_image(img_path_read, data_dir, category, cat_it, dataset_name, train_iteration_no, 'train', resolution)\n",
        "                train_iteration_no = train_iteration_no + 1\n",
        "\n",
        "            #Save val images\n",
        "            for img_path_read in imgs_path_val:\n",
        "                val_iteration_no = val_iteration_no + 1\n",
        "                save_image(img_path_read, data_dir, category, cat_it, dataset_name, val_iteration_no, 'val', resolution)\n",
        "\n",
        "    temp_paths_train = os.listdir(os.path.join(data_dir, 'train'))\n",
        "    temp_paths_train = [os.path.join(dataset_dir, dataset_exp_name, images_folder_name, 'train', x) for x in temp_paths_train]\n",
        "    imgs_path_train = np.array(temp_paths_train)\n",
        "\n",
        "    temp_paths_val = os.listdir(os.path.join(data_dir, 'val'))\n",
        "    temp_paths_val = [os.path.join(dataset_dir, dataset_exp_name, images_folder_name, 'val', x) for x in temp_paths_val]\n",
        "    imgs_path_val = np.array(temp_paths_val)\n",
        "\n",
        "    if not os.path.exists(os.path.join(server_root_path, dataset_dir, dataset_exp_name, 'index_lists')):\n",
        "        os.mkdir(os.path.join(server_root_path, dataset_dir, dataset_exp_name, 'index_lists'))\n",
        "\n",
        "    #Save index list train\n",
        "    save_path = os.path.join(server_root_path, dataset_dir, dataset_exp_name, 'index_lists')\n",
        "    np.save(save_path + '/' + images_folder_name + '_index_list_' + 'train.npy',imgs_path_train)\n",
        "\n",
        "    #Save index list val\n",
        "    save_path = os.path.join(server_root_path, dataset_dir, dataset_exp_name, 'index_lists')\n",
        "    np.save(save_path + '/' + images_folder_name + '_index_list_' + 'val.npy', imgs_path_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJy3PIkxpqNP",
        "outputId": "e185efe7-bf1d-46cb-c263-782a2920a47c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running usfda_office_31_DtoA\n",
            "shared_classes: ['back_pack', 'calculator', 'keyboard', 'monitor', 'mouse', 'mug', 'bike', 'laptop_computer', 'headphones', 'projector']\n",
            "source_private_classes: ['bike_helmet', 'bookcase', 'bottle', 'desk_chair', 'desk_lamp', 'desktop_computer', 'file_cabinet', 'letter_tray', 'mobile_phone', 'paper_notebook']\n",
            "target_private_classes: ['pen', 'phone', 'printer', 'punchers', 'ring_binder', 'ruler', 'scissors', 'speaker', 'stapler', 'tape_dispenser', 'trash_can']\n",
            "/content/drive/MyDrive/Office-31/usfda_office_31_DtoA/source_images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "  5%|▌         | 1/20 [00:00<00:12,  1.50it/s]\u001b[A\n",
            " 10%|█         | 2/20 [00:01<00:11,  1.56it/s]\u001b[A\n",
            " 15%|█▌        | 3/20 [00:01<00:09,  1.73it/s]\u001b[A\n",
            " 20%|██        | 4/20 [00:02<00:12,  1.31it/s]\u001b[A\n",
            " 25%|██▌       | 5/20 [00:03<00:10,  1.47it/s]\u001b[A\n",
            " 30%|███       | 6/20 [00:03<00:08,  1.73it/s]\u001b[A\n",
            " 35%|███▌      | 7/20 [00:04<00:08,  1.45it/s]\u001b[A\n",
            " 40%|████      | 8/20 [00:05<00:09,  1.24it/s]\u001b[A\n",
            " 45%|████▌     | 9/20 [00:06<00:08,  1.35it/s]\u001b[A\n",
            " 50%|█████     | 10/20 [00:07<00:08,  1.20it/s]\u001b[A\n",
            " 55%|█████▌    | 11/20 [00:08<00:08,  1.04it/s]\u001b[A\n",
            " 60%|██████    | 12/20 [00:09<00:06,  1.21it/s]\u001b[A\n",
            " 65%|██████▌   | 13/20 [00:09<00:05,  1.21it/s]\u001b[A\n",
            " 70%|███████   | 14/20 [00:10<00:04,  1.30it/s]\u001b[A\n",
            " 75%|███████▌  | 15/20 [00:11<00:03,  1.38it/s]\u001b[A\n",
            " 80%|████████  | 16/20 [00:11<00:02,  1.39it/s]\u001b[A\n",
            " 85%|████████▌ | 17/20 [00:12<00:02,  1.34it/s]\u001b[A\n",
            " 90%|█████████ | 18/20 [00:13<00:01,  1.28it/s]\u001b[A\n",
            " 95%|█████████▌| 19/20 [00:15<00:01,  1.05s/it]\u001b[A\n",
            "100%|██████████| 20/20 [00:15<00:00,  1.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "  5%|▍         | 1/21 [00:03<01:07,  3.35s/it]\u001b[A\n",
            " 10%|▉         | 2/21 [00:06<00:59,  3.15s/it]\u001b[A\n",
            " 14%|█▍        | 3/21 [00:09<00:56,  3.13s/it]\u001b[A\n",
            " 19%|█▉        | 4/21 [00:13<00:56,  3.32s/it]\u001b[A\n",
            " 24%|██▍       | 5/21 [00:16<00:54,  3.43s/it]\u001b[A\n",
            " 29%|██▊       | 6/21 [00:19<00:50,  3.36s/it]\u001b[A\n",
            " 33%|███▎      | 7/21 [00:22<00:43,  3.12s/it]\u001b[A\n",
            " 38%|███▊      | 8/21 [00:26<00:41,  3.23s/it]\u001b[A\n",
            " 43%|████▎     | 9/21 [00:29<00:39,  3.30s/it]\u001b[A\n",
            " 48%|████▊     | 10/21 [00:32<00:34,  3.16s/it]\u001b[A\n",
            " 52%|█████▏    | 11/21 [00:35<00:30,  3.04s/it]\u001b[A\n",
            " 57%|█████▋    | 12/21 [00:38<00:27,  3.07s/it]\u001b[A\n",
            " 62%|██████▏   | 13/21 [00:41<00:25,  3.20s/it]\u001b[A\n",
            " 67%|██████▋   | 14/21 [00:45<00:22,  3.26s/it]\u001b[A\n",
            " 71%|███████▏  | 15/21 [00:48<00:19,  3.21s/it]\u001b[A\n",
            " 76%|███████▌  | 16/21 [00:50<00:14,  2.88s/it]\u001b[A/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/category_scissors_category_number_16_dataset_amazon_1531.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "\n",
            " 81%|████████  | 17/21 [00:53<00:11,  2.99s/it]\u001b[A\n",
            " 86%|████████▌ | 18/21 [00:57<00:09,  3.16s/it]\u001b[A\n",
            " 90%|█████████ | 19/21 [01:00<00:06,  3.20s/it]\u001b[A\n",
            " 95%|█████████▌| 20/21 [01:03<00:03,  3.14s/it]\u001b[A\n",
            "100%|██████████| 21/21 [01:05<00:00,  3.12s/it]\n",
            "100%|██████████| 1/1 [01:21<00:00, 81.95s/it]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from natsort import natsorted\n",
        "\n",
        "#Place where all data is stored\n",
        "dataset_dir = 'Office-31'\n",
        "\n",
        "dataset_exp_names = ['usfda_office_31_DtoA']\n",
        "datasets_sources = [['dslr']]\n",
        "datasets_targets = ['amazon']\n",
        "\n",
        "C = ['back_pack', 'calculator', 'keyboard', 'monitor', 'mouse', 'mug', 'bike', 'laptop_computer', 'headphones', 'projector']\n",
        "Cs_dash = ['bike_helmet', 'bookcase', 'bottle', 'desk_chair', 'desk_lamp', 'desktop_computer', 'file_cabinet', 'letter_tray', 'mobile_phone', 'paper_notebook']\n",
        "Ct_dash = ['pen', 'phone', 'printer', 'punchers', 'ring_binder', 'ruler', 'scissors', 'speaker', 'stapler', 'tape_dispenser', 'trash_can']\n",
        "\n",
        "#number of shared classes between source and target\n",
        "num_shared_classes = len(C)\n",
        "\n",
        "#number of unknown classes in target domain\n",
        "num_unknown_target_classes = len(Cs_dash)\n",
        "\n",
        "#number of unknown classes in source domain\n",
        "num_unknown_source_classes = len(Ct_dash)\n",
        "\n",
        "for dataset_exp_name, datasets_source, datasets_target in tqdm(list(zip(dataset_exp_names, datasets_sources, datasets_targets))):\n",
        "\n",
        "    print('running', dataset_exp_name)\n",
        "\n",
        "    if dataset_exp_name.split('_')[-1] == 'saito':\n",
        "      resolution = 32\n",
        "    else:\n",
        "      resolution = 224\n",
        "\n",
        "    source_train_val_split = 0.9\n",
        "    target_train_val_split = 1\n",
        "\n",
        "    # Create a folder inside the dataset experiment folder server_root_path, dataset_dir, dataset_exp_name\n",
        "    if not os.path.exists(os.path.join(server_root_path, dataset_dir, dataset_exp_name)):\n",
        "        os.mkdir(os.path.join(server_root_path, dataset_dir, dataset_exp_name))\n",
        "    else:\n",
        "        os.system('rm -rf ' + os.path.join(server_root_path, dataset_dir, dataset_exp_name) + '/*')\n",
        "\n",
        "    num_datasets = len(datasets_source) + 1\n",
        "    all_datasets = datasets_source + [datasets_target]\n",
        "\n",
        "    print('shared_classes: {}'.format(C))\n",
        "    print('source_private_classes: {}'.format(Cs_dash))\n",
        "    print('target_private_classes: {}'.format(Ct_dash))\n",
        "\n",
        "    #Create Source Data\n",
        "    save_data(server_root_path, dataset_dir, dataset_exp_name, 'source_images', datasets_source, C, Cs_dash, source_train_val_split, resolution=resolution)\n",
        "\n",
        "    #Create Target Data\n",
        "    save_data(server_root_path, dataset_dir, dataset_exp_name, 'target_images', [datasets_target], C, Ct_dash, target_train_val_split, resolution=resolution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dzR8ONTY5f3U"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "from tqdm import tqdm\n",
        "from scipy.interpolate import interp1d\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "import torch\n",
        "from natsort import natsorted\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# Set device to GPU if available, otherwise CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b-dN1OYqLJD",
        "outputId": "370a4b3b-dddd-4b01-feb4-06a302256186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running usfda_office_31_DtoA\n",
            "shared_classes: ['back_pack', 'calculator', 'keyboard', 'monitor', 'mouse', 'mug', 'bike', 'laptop_computer', 'headphones', 'projector']\n",
            "source_private_classes: ['bike_helmet', 'bookcase', 'bottle', 'desk_chair', 'desk_lamp', 'desktop_computer', 'file_cabinet', 'letter_tray', 'mobile_phone', 'paper_notebook']\n",
            "target_private_classes: ['pen', 'phone', 'printer', 'punchers', 'ring_binder', 'ruler', 'scissors', 'speaker', 'stapler', 'tape_dispenser', 'trash_can']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing negative images: 100%|███████████████████████████████| 5000/5000 [14:46<00:00,  5.64it/s]\n"
          ]
        }
      ],
      "source": [
        "# 2-way or 3-way splicing\n",
        "n_way_splice = 2\n",
        "\n",
        "#Place where all data is stored\n",
        "# dataset_dir = 'data/digits'\n",
        "dataset_dir = 'Office-31'\n",
        "\n",
        "dataset_exp_names = ['usfda_office_31_DtoA']\n",
        "datasets_sources = [['dslr']]\n",
        "datasets_targets = ['amazon']\n",
        "\n",
        "C = ['back_pack', 'calculator', 'keyboard', 'monitor', 'mouse', 'mug', 'bike', 'laptop_computer', 'headphones', 'projector']\n",
        "Cs_dash = ['bike_helmet', 'bookcase', 'bottle', 'desk_chair', 'desk_lamp', 'desktop_computer', 'file_cabinet', 'letter_tray', 'mobile_phone', 'paper_notebook']\n",
        "Ct_dash = ['pen', 'phone', 'printer', 'punchers', 'ring_binder', 'ruler', 'scissors', 'speaker', 'stapler', 'tape_dispenser', 'trash_can']\n",
        "\n",
        "#number of shared classes between source and target\n",
        "num_shared_classes = len(C)\n",
        "\n",
        "#number of unknown classes in target domain\n",
        "num_unknown_target_classes = len(Cs_dash)\n",
        "\n",
        "#number of unknown classes in source domain\n",
        "num_unknown_source_classes = len(Ct_dash)\n",
        "\n",
        "# Negative categories\n",
        "num_source_classes = len(C) + len(Cs_dash)\n",
        "temp_negative_category_dict = {}\n",
        "c = 0\n",
        "for i in range(num_source_classes):\n",
        "\tfor j in range(i+1, num_source_classes):\n",
        "\t\ttemp_negative_category_dict[(i, j)] = c\n",
        "\t\tc += 1\n",
        "\n",
        "negative_category_dict = {}\n",
        "\n",
        "for key in temp_negative_category_dict:\n",
        "\tnegative_category_dict[key] = temp_negative_category_dict[key]\n",
        "\tnegative_category_dict[(key[1], key[0])] = temp_negative_category_dict[key]\n",
        "\n",
        "\n",
        "def generate_spline(num_points, points = None, vertical = False, integer = False):\n",
        "\n",
        "\t'''\n",
        "\t\tPerforms spline interpolation between points, and returns num_points coordinates.\n",
        "\t\tArgs:\n",
        "\t\t\tpoints: list of (y,x) coordinates. If none, then samples from num_points\n",
        "\t\t\tvertical: True if points are y coordinates (False if points are x coordinates)\n",
        "\t\t\tinteger: True if the function should return integer points\n",
        "\t'''\n",
        "\n",
        "\tHMAX, WMAX = num_points, num_points\n",
        "\twindow = 30\n",
        "\tcenter = (HMAX/2, WMAX/2)\n",
        "\n",
        "\trand_center_point = (np.random.randint(low=int(center[0]-window), high=int(center[0]+window)), np.random.randint(low=int(center[1]-(window/3)), high=int(center[1]+(window/3))))\n",
        "\n",
        "\tpoints = [(0, np.random.randint(HMAX)), rand_center_point, (WMAX-1, np.random.randint(HMAX))]\n",
        "\n",
        "\tpx = [p[0] for p in points]\n",
        "\tpx.extend([0, 223])\n",
        "\n",
        "\tpy = [p[1] for p in points]\n",
        "\tpy.extend([0, 223])\n",
        "\n",
        "\t# spl = spline_interpolation([p[0] for p in points], [p[1] for p in points], range(num_points), order=7, kind='smoothest')\n",
        "\tspl = interp1d([p[0] for p in points], [p[1] for p in points], kind='quadratic')(range(num_points))\n",
        "\n",
        "\tx = list(range(num_points))\n",
        "\ty = [p for p in spl]\n",
        "\n",
        "\tif integer:\n",
        "\t\tx = [int(a) for a in x]\n",
        "\t\ty = [int(a) for a in y]\n",
        "\n",
        "\tif not(vertical):\n",
        "\t\treturn (x, y), [px, py]\n",
        "\telse:\n",
        "\t\treturn (y, x), [py, px]\n",
        "\n",
        "\n",
        "def get_negative_image(image1, image2, spline = None, vertical_division = False):\n",
        "\n",
        "\t'''\n",
        "\t\tReturns 2 negative images by merging 2 images horizontally or vertically (vertical_division).\n",
        "\t'''\n",
        "\n",
        "\tif not(vertical_division):\n",
        "\n",
        "\t\tmask1 = torch.ones(image1.shape).to(device) # Image to the left of spline\n",
        "\t\tfor (x, y) in spline:\n",
        "\t\t\tmask1[y:, x] = 0\n",
        "\t\tmask2 = 1 - mask1 # Image to the right spline\n",
        "\n",
        "\t\t# print(mask1)\n",
        "\t\t# print(mask2)\n",
        "\n",
        "\t\t# io.imsave('temp_mask1.jpg', (mask1.cpu().numpy() * 255).astype(np.uint8))\n",
        "\t\t# io.imsave('temp_mask2.jpg', (mask2.cpu().numpy() * 255).astype(np.uint8))\n",
        "\n",
        "\t\tmask1 = mask1.float()\n",
        "\t\tmask2 = mask2.float()\n",
        "\n",
        "\t\tnewim1 = mask1 * image1 + mask2 * image2\n",
        "\t\tnewim2 = mask1 * image2 + mask2 * image1\n",
        "\n",
        "\t\treturn newim1, newim2, mask1, mask2\n",
        "\n",
        "\telse:\n",
        "\n",
        "\t\tmask1 = torch.ones(image1.shape).to(device) # Image above spline\n",
        "\t\tfor (x, y) in spline:\n",
        "\t\t\tmask1[y, x:] = 0\n",
        "\t\tmask2 = 1 - mask1 # Image below spline\n",
        "\n",
        "\t\t# print(mask1)\n",
        "\t\t# print(mask2)\n",
        "\n",
        "\t\t# io.imsave('temp_mask1.jpg', (mask1.cpu().numpy() * 255).astype(np.uint8))\n",
        "\t\t# io.imsave('temp_mask2.jpg', (mask2.cpu().numpy() * 255).astype(np.uint8))\n",
        "\n",
        "\t\tmask1 = mask1.float()\n",
        "\t\tmask2 = mask2.float()\n",
        "\n",
        "\t\tnewim1 = mask1 * image1 + mask2 * image2\n",
        "\t\tnewim2 = mask1 * image2 + mask2 * image1\n",
        "\n",
        "\t\treturn newim1, newim2, mask1, mask2\n",
        "\n",
        "\n",
        "def merge_images(image1, image2):\n",
        "\n",
        "\tassert image1.shape == image2.shape\n",
        "\n",
        "\timage1, image2 = torch.from_numpy(image1).to(device), torch.from_numpy(image2).to(device)\n",
        "\timage1, image2 = image1.float(), image2.float()\n",
        "\n",
        "\tvert = False\n",
        "\t(x, y), _ = generate_spline(image1.shape[0], vertical=vert, integer=True)\n",
        "\tspline = list(zip(x, y))\n",
        "\thor_I1, hor_I2, hor_mask1, hor_mask2 = get_negative_image(image1, image2, spline, vertical_division=vert)\n",
        "\n",
        "\tvert = True\n",
        "\t(x, y), _ = generate_spline(image1.shape[0], vertical=vert, integer=True)\n",
        "\tspline = list(zip(x, y))\n",
        "\tver_I1, ver_I2, ver_mask1, ver_mask2 = get_negative_image(image1, image2, spline, vertical_division=vert)\n",
        "\n",
        "\treturn hor_I1, hor_I2, ver_I1, ver_I2, hor_mask1, hor_mask2, ver_mask1, ver_mask2\n",
        "\n",
        "\n",
        "def get_negative_image_3(image1, image2, image3, spline_vert, spline_hor):\n",
        "\n",
        "\t'''\n",
        "\t\tMerges 3 images, in a 3-way splicing.\n",
        "\n",
        "\t\tspline_vert -> vertical spline (going from top to bottom edge)\n",
        "\t\tspline_hor -> horizontal spline (going from left to right edge)\n",
        "\n",
        "\t\timage1 -> class A\n",
        "\t\timage2 -> class A\n",
        "\t\timage3 -> class B\n",
        "\n",
        "\t\tThe following 4 scenarios can arise\n",
        "\n",
        "\t\t\tA | A   \tA |       \t  | A     \t  B\n",
        "\t\t1)  -----   2)\t--| B   3)\tB |--   4)\t-----\n",
        "\t\t\t  B     \tA |       \t  | A   \tA | A\n",
        "\n",
        "\t\tThe corresponding operations are:\n",
        "\n",
        "\t\t1) spline_vert_left * image1 + spline_vert_right * image2 + spline_hor_bottom * image3\n",
        "\t\t2) spline_hor_top * image1 + spline_hor_bottom * image2 + spline_vert_right * image3\n",
        "\t\t3) spline_hor_top * image1 + spline_hor_bottom * image2 + spline_vert_left * image3\n",
        "\t\t4) spline_vert_left * image1 + spline_vert_right * image2 + spline_hor_top * image3\n",
        "\n",
        "\t\tOut of these, some will not have enough representation from one class\n",
        "\t'''\n",
        "\n",
        "\tspline_vert_left = torch.ones(image1.shape).to(device) # Image to the left of spline\n",
        "\tfor (x, y) in spline_vert:\n",
        "\t\tspline_vert_left[y, x:] = 0\n",
        "\tspline_vert_right = 1 - spline_vert_left # Image to the right spline\n",
        "\n",
        "\tspline_hor_top = torch.ones(image1.shape).to(device) # Image to the top of spline\n",
        "\tfor (x, y) in spline_hor:\n",
        "\t\tspline_hor_top[y:, x] = 0\n",
        "\tspline_hor_bottom = 1 - spline_hor_top # Image to the bottom of spline\n",
        "\n",
        "\tI1 = spline_hor_top * (spline_vert_left * image1 + spline_vert_right * image2) + spline_hor_bottom * image3\n",
        "\tI2 = spline_vert_left * (spline_hor_top * image1 + spline_hor_bottom * image2) + spline_vert_right * image3\n",
        "\tI3 = spline_vert_right * (spline_hor_top * image1 + spline_hor_bottom * image2) + spline_vert_left * image3\n",
        "\tI4 = spline_hor_bottom * (spline_vert_left * image1 + spline_vert_right * image2) + spline_hor_top * image3\n",
        "\n",
        "\tM1 = spline_hor_top * (spline_vert_left * torch.ones(image1.shape).to(device) * 32 + spline_vert_right * torch.ones(image1.shape).to(device) * 64) + spline_hor_bottom * torch.ones(image1.shape).to(device) * 192\n",
        "\tM2 = spline_vert_left * (spline_hor_top * torch.ones(image1.shape).to(device) * 32 + spline_hor_bottom * torch.ones(image1.shape).to(device) * 64) + spline_vert_right * torch.ones(image1.shape).to(device) * 192\n",
        "\tM3 = spline_vert_right * (spline_hor_top * torch.ones(image1.shape).to(device) * 32 + spline_hor_bottom * torch.ones(image1.shape).to(device) * 64) + spline_vert_left * torch.ones(image1.shape).to(device) * 192\n",
        "\tM4 = spline_hor_bottom * (spline_vert_left * torch.ones(image1.shape).to(device) * 32 + spline_vert_right * torch.ones(image1.shape).to(device) * 64) + spline_hor_top * torch.ones(image1.shape).to(device) * 192\n",
        "\n",
        "\treturn I1, I2, I3, I4, M1, M2, M3, M4\n",
        "\n",
        "\n",
        "def merge_images_3(image1, image2, image3):\n",
        "\n",
        "\t(x, y), _ = generate_spline(image1.shape[0], vertical=True, integer=True)\n",
        "\tspline_vert = list(zip(x, y))\n",
        "\n",
        "\t(x, y), _ = generate_spline(image1.shape[0], vertical=False, integer=True)\n",
        "\tspline_hor = list(zip(x, y))\n",
        "\n",
        "\timage1, image2, image3 = torch.from_numpy(image1).to(device), torch.from_numpy(image2).to(device), torch.from_numpy(image3).to(device)\n",
        "\timage1, image2, image3 = image1.float(), image2.float(), image3.float()\n",
        "\n",
        "\treturn get_negative_image_3(image1, image2, image3, spline_vert, spline_hor)\n",
        "\n",
        "\n",
        "def get_category_number(c1, c2):\n",
        "\n",
        "\t'''\n",
        "\t\tReturns the category number given two classes.\n",
        "\t'''\n",
        "\n",
        "\treturn negative_category_dict[(c1, c2)]\n",
        "\n",
        "\n",
        "for dataset_exp_name, datasets_source, datasets_target in zip(dataset_exp_names, datasets_sources, datasets_targets):\n",
        "    print('running', dataset_exp_name)\n",
        "\n",
        "    print('shared_classes: {}'.format(C))\n",
        "    print('source_private_classes: {}'.format(Cs_dash))\n",
        "    print('target_private_classes: {}'.format(Ct_dash))\n",
        "\n",
        "    filenames = glob(os.path.join(server_root_path, dataset_dir, dataset_exp_name, 'source_images', 'train', '*.png'))\n",
        "    savepath = os.path.join(server_root_path, dataset_dir, dataset_exp_name, 'negative_images')\n",
        "    savepath_mask = os.path.join(server_root_path, dataset_dir, dataset_exp_name, 'negative_masks')\n",
        "\n",
        "    if os.path.exists(savepath):\n",
        "        os.system('rm -rf ' + savepath)\n",
        "    os.mkdir(savepath)\n",
        "\n",
        "    if os.path.exists(savepath_mask):\n",
        "        os.system('rm -rf ' + savepath_mask)\n",
        "    os.mkdir(savepath_mask)\n",
        "\n",
        "    # Remove augmented files\n",
        "    # filenames = [a for a in filenames if a.split('/')[-1].split('_')[0]=='category']\n",
        "    L = len(filenames)\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    class_wise_files = {}\n",
        "\n",
        "    if n_way_splice == 3:\n",
        "        for cnum in range(num_source_classes):\n",
        "            class_wise_files[str(cnum)] = []\n",
        "            pattern = re.compile('_category_number_' + str(cnum) + '_dataset_' + str(datasets_source[0]) + '_')\n",
        "            for fname in filenames:\n",
        "                if pattern.search(fname) is not None:\n",
        "                    class_wise_files[str(cnum)].append(fname)\n",
        "\n",
        "    for i in tqdm(range(5000), desc=\"Processing negative images\", ncols=100):  # Wrap the inner loop with tqdm\n",
        "\n",
        "        if n_way_splice == 2:\n",
        "            im1 = np.random.randint(L)\n",
        "            im2 = np.random.randint(L)\n",
        "\n",
        "            f1, f2 = filenames[im1], filenames[im2]\n",
        "            c1, c2 = int(f1.split('_')[-4]), int(f2.split('_')[-4])\n",
        "\n",
        "            while c1 == c2:\n",
        "                im2 = np.random.randint(L)\n",
        "                f2 = filenames[im2]\n",
        "                c2 = int(f2.split('_')[-4])\n",
        "\n",
        "            image1, image2 = io.imread(f1), io.imread(f2)\n",
        "\n",
        "            a, b, c, d, e, f, g, h = merge_images(image1, image2)\n",
        "            merged_images = [a, b, c, d]\n",
        "            merged_masks = [e, f, g, h]\n",
        "\n",
        "            cnum = get_category_number(c1, c2)\n",
        "\n",
        "            for image, mask in zip(merged_images, merged_masks):\n",
        "                counter += 1\n",
        "                save_filename = 'category_' + str(cnum) + '_category_number_' + str(cnum) + '_dataset_' + str(datasets_source[0]) + '_' + str(counter) + '.png'\n",
        "                save_filename_mask = 'mask_category_' + str(cnum) + '_category_number_' + str(cnum) + '_dataset_' + str(datasets_source[0]) + '_' + str(counter) + '.png'\n",
        "                io.imsave(os.path.join(savepath, save_filename), image.cpu().numpy().astype(np.uint8))\n",
        "                io.imsave(os.path.join(savepath_mask, save_filename_mask), (mask * 255).cpu().numpy().astype(np.uint8))\n",
        "\n",
        "        elif n_way_splice == 3:\n",
        "            # first image\n",
        "            im1 = np.random.randint(L)\n",
        "            f1 = filenames[im1]\n",
        "            c1 = int(f1.split('_')[-4])\n",
        "\n",
        "            # second image\n",
        "            im2 = np.random.randint(len(class_wise_files[str(c1)]))\n",
        "            f2 = class_wise_files[str(c1)][im2]\n",
        "            c2 = c1\n",
        "\n",
        "            while f1 == f2:\n",
        "                im2 = np.random.randint(len(class_wise_files[str(c1)]))\n",
        "                f2 = class_wise_files[str(c1)][im2]\n",
        "\n",
        "            # third image\n",
        "            x = np.random.randint(len(list(class_wise_files.keys())))\n",
        "            c3 = int((list(class_wise_files.keys())[x]))\n",
        "\n",
        "            while c3 == c2:\n",
        "                x = np.random.randint(len(list(class_wise_files.keys())))\n",
        "                c3 = int((list(class_wise_files.keys())[x]))\n",
        "\n",
        "            im3 = np.random.randint(len(class_wise_files[str(c3)]))\n",
        "            f3 = class_wise_files[str(c3)][im3]\n",
        "\n",
        "            image1, image2, image3 = io.imread(f1), io.imread(f2), io.imread(f3)\n",
        "            i1, i2, i3, i4, m1, m2, m3, m4 = merge_images_3(image1, image2, image3)\n",
        "            merged_images = [i1, i2, i3, i4]\n",
        "            merged_masks = [m1, m2, m3, m4]\n",
        "\n",
        "            cnum = get_category_number(c1, c3)\n",
        "\n",
        "            for image, mask in zip(merged_images, merged_masks):\n",
        "                counter += 1\n",
        "                save_filename = 'category_' + str(cnum) + '_category_number_' + str(cnum) + '_dataset_' + str(datasets_source[0]) + '_' + str(counter) + '.png'\n",
        "                save_filename_mask = 'mask_category_' + str(cnum) + '_category_number_' + str(cnum) + '_dataset_' + str(datasets_source[0]) + '_' + str(counter) + '.png'\n",
        "                io.imsave(os.path.join(savepath, save_filename), image.cpu().numpy().astype(np.uint8))\n",
        "                io.imsave(os.path.join(savepath_mask, save_filename_mask), (mask).cpu().numpy().astype(np.uint8))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = 'Office-31'\n",
        "dataset_exp_name = 'usfda_office_31_DtoA'\n",
        "images_folder_name = 'negative_images'\n",
        "\n",
        "data_dir = os.path.join(server_root_path, dataset_dir, dataset_exp_name, images_folder_name)\n",
        "temp_paths_train = os.listdir(data_dir)\n",
        "\n",
        "temp_paths_train = [os.path.join(dataset_dir, dataset_exp_name, images_folder_name, x) for x in temp_paths_train]\n",
        "imgs_path_train = np.array(temp_paths_train)\n",
        "\n",
        "print(data_dir)\n",
        "print(len(temp_paths_train))\n",
        "print(len(imgs_path_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSG4KkThDXSx",
        "outputId": "e360b059-e30c-44bd-b308-b9d1e69083a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Office-31/usfda_office_31_DtoA/negative_images\n",
            "8733\n",
            "8733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save index list train\n",
        "save_path = os.path.join(server_root_path, dataset_dir, dataset_exp_name, 'index_lists')\n",
        "np.save(save_path + '/' + images_folder_name + '_index_list_' + 'train.npy', imgs_path_train)"
      ],
      "metadata": {
        "id": "on7lvrj3Etqm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import glob\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "chop_distances = {}\n",
        "\n",
        "def get_chop_distance(rotated_mat, angle):\n",
        "\n",
        "    '''\n",
        "        Returns the distance at which the rotated image should be chopped off. Used in rotateImage() function.\n",
        "    '''\n",
        "\n",
        "    global chop_distances\n",
        "\n",
        "    if angle in chop_distances.keys():\n",
        "        return chop_distances[angle]\n",
        "\n",
        "    if angle > 0:\n",
        "\n",
        "        x = 0\n",
        "        y = 0\n",
        "\n",
        "        while(rotated_mat[y, x, 0] == 0):\n",
        "            y += 1\n",
        "\n",
        "        chop_distances[angle] = y\n",
        "\n",
        "        # print(chop_distances)\n",
        "        return y\n",
        "\n",
        "    else:\n",
        "\n",
        "        x = rotated_mat.shape[1] - 1\n",
        "        y = 0\n",
        "\n",
        "        while(rotated_mat[y, x, 0] == 0):\n",
        "            y += 1\n",
        "\n",
        "        chop_distances[angle] = y\n",
        "\n",
        "        # print(chop_distances)\n",
        "        return y\n",
        "\n",
        "\n",
        "def rotateImage(mat, angle):\n",
        "\n",
        "    '''\n",
        "        Rotates an image (angle in degrees) and zooms into the image to avoid crop borders.\n",
        "    '''\n",
        "\n",
        "    height, width = mat.shape[:2] # image shape has 3 dimensions\n",
        "    image_center = (width/2, height/2) # getRotationMatrix2D needs coordinates in reverse order (width, height) compared to shape\n",
        "\n",
        "    rotation_mat = cv2.getRotationMatrix2D(image_center, angle, 1.)\n",
        "\n",
        "    # rotation calculates the cos and sin, taking absolutes of those.\n",
        "    abs_cos = abs(rotation_mat[0,0])\n",
        "    abs_sin = abs(rotation_mat[0,1])\n",
        "\n",
        "    # find the new width and height bounds\n",
        "    bound_w = int(height * abs_sin + width * abs_cos)\n",
        "    bound_h = int(height * abs_cos + width * abs_sin)\n",
        "\n",
        "    # subtract old image center (bringing image back to origo) and adding the new image center coordinates\n",
        "    rotation_mat[0, 2] += bound_w/2 - image_center[0]\n",
        "    rotation_mat[1, 2] += bound_h/2 - image_center[1]\n",
        "\n",
        "    # rotate image with the new bounds and translated rotation matrix\n",
        "    rotated_mat = cv2.warpAffine(mat, rotation_mat, (bound_w, bound_h))\n",
        "\n",
        "    H, W, c = rotated_mat.shape\n",
        "\n",
        "    d = get_chop_distance(rotated_mat, angle)\n",
        "    chopped_image = rotated_mat[d : H-d, d : W-d]\n",
        "\n",
        "    resized_image = cv2.resize(chopped_image, (224, 224))\n",
        "\n",
        "    return resized_image\n",
        "\n",
        "\n",
        "def random_crop(image, cropshape, padsize):\n",
        "\n",
        "    '''\n",
        "        Takes a random crop from the image.\n",
        "    '''\n",
        "\n",
        "    # crop height, width, channels\n",
        "    H, W, C = image.shape\n",
        "    p = padsize\n",
        "\n",
        "    # Check shapes etc.\n",
        "    if type(cropshape) == int:\n",
        "        cH = cW = cropshape\n",
        "        assert cH <= H, 'Crop size is greater than image size'\n",
        "    elif len(cropshape) == 2:\n",
        "        cH, cW = cropshape\n",
        "        assert cH <= H and cW <= W, 'Crop size is greater than image size'\n",
        "    else:\n",
        "        raise Exception('Wrong crop shape (use either int (s) or tuple (h, w))')\n",
        "\n",
        "    if type(padsize) == int:\n",
        "        pH = pW = padsize\n",
        "    elif len(padsize) == 2:\n",
        "        pH, pW = padsize\n",
        "    else:\n",
        "        raise Exception('Wrong pad shape (use either int (s) or tuple (h, w))')\n",
        "\n",
        "    # Created padded image\n",
        "    paddedimage = np.zeros((cH + 2*pH, cW + 2*pW, C), dtype = image.dtype)\n",
        "    paddedimage[pH:pH+H, pW:pW+W, :] = image\n",
        "\n",
        "    # Output image\n",
        "    outimage = np.zeros((cH, cW, C), dtype = image.dtype)\n",
        "\n",
        "    # Randomly chose a start location (this is the random step)\n",
        "    startx = np.random.randint(2*padsize)\n",
        "    starty = np.random.randint(2*padsize)\n",
        "\n",
        "    # Crop (H, W, C) shaped image from start locations\n",
        "    outimage = paddedimage[starty:starty+cH, startx:startx+cH, :]\n",
        "\n",
        "    return outimage\n",
        "\n",
        "\n",
        "def random_horizontal_flip(image, always_flip=True):\n",
        "\n",
        "    '''\n",
        "        Randomly flips the given image.\n",
        "    '''\n",
        "\n",
        "    if always_flip:\n",
        "        r = 1 # always flip\n",
        "    else:\n",
        "        r = np.random.rand()\n",
        "\n",
        "    if r >= 0:\n",
        "        return np.fliplr(image), True\n",
        "    else:\n",
        "        return np.array(image), False\n",
        "\n",
        "\n",
        "def rgb_flip(img, reorder):\n",
        "\n",
        "    '''\n",
        "        Flips the channels in the given order.\n",
        "    '''\n",
        "\n",
        "    newim = img.copy()\n",
        "    newim[:, :, 0] = img[:, :, reorder[0]]\n",
        "    newim[:, :, 1] = img[:, :, reorder[1]]\n",
        "    newim[:, :, 2] = img[:, :, reorder[2]]\n",
        "\n",
        "    return newim\n",
        "\n",
        "\n",
        "def color_jitter(img, brightness=0.25, contrast=0.25, saturation=0.25, hue=0.25):\n",
        "\n",
        "    '''\n",
        "        Applies color jitter augmentation (brightness, contrast, saturation, hue).\n",
        "    '''\n",
        "\n",
        "    tensorImage = torchvision.transforms.ToTensor()(img)\n",
        "    pilImage = torchvision.transforms.ToPILImage(mode='RGB')(tensorImage)\n",
        "    jitteredImage = torchvision.transforms.ColorJitter(brightness, contrast, saturation, hue)(pilImage)\n",
        "    jitteredArray = np.array(jitteredImage)\n",
        "\n",
        "    return jitteredArray\n",
        "\n",
        "\n",
        "def count_class_distribution(images_path):\n",
        "\n",
        "    '''\n",
        "        Get the number of images in each class. Call before augmenting dataset so that each class has same number of examples.\n",
        "    '''\n",
        "\n",
        "    classdict = {}\n",
        "\n",
        "    for im in images_path:\n",
        "\n",
        "        a = im.split('/')[-1]\n",
        "        c = int(a.split('category_number_')[1].split('_')[0])\n",
        "\n",
        "        if c in classdict:\n",
        "            classdict[c].append(im)\n",
        "        else:\n",
        "            classdict[c] = [im]\n",
        "\n",
        "    return classdict\n",
        "\n",
        "\n",
        "def augment_images(images_path):\n",
        "\n",
        "    '''\n",
        "        Main function to augment all images with filenames given in images_path.\n",
        "    '''\n",
        "\n",
        "    # print('\\nAugment Images')\n",
        "\n",
        "    # Rotate and augment first\n",
        "    for xx in images_path:\n",
        "\n",
        "        image = io.imread(xx)\n",
        "\n",
        "        augmentated_images = []\n",
        "\n",
        "        # Flip image\n",
        "        image_flip, success = random_horizontal_flip(image)\n",
        "        if success:\n",
        "            file_name = '/'.join(xx.split('/')[:-1]) + '/' + 'flip_' +  xx.split('/')[-1]\n",
        "            augmentated_images.append(file_name)\n",
        "            io.imsave(file_name, image_flip)\n",
        "\n",
        "        # Rotate Image\n",
        "        for yy in [-3, -2, -1, 1, 2, 3]:\n",
        "            image_rot = rotateImage(image, yy*5)\n",
        "            file_name = '/'.join(xx.split('/')[:-1]) + '/' + 'rotate' + str(yy) + '_' +  xx.split('/')[-1]\n",
        "            io.imsave(file_name, image_rot)\n",
        "\n",
        "        # RGB flip\n",
        "        for order in [(0, 2, 1), (1, 0, 2), (1, 2, 0), (2, 0, 1), (2, 1, 0)]:\n",
        "            r = str(order[0])\n",
        "            g = str(order[1])\n",
        "            b = str(order[2])\n",
        "            image_rgb_flipped = rgb_flip(image, order)\n",
        "            file_name = '/'.join(xx.split('/')[:-1]) + '/' + 'rgbflip' + r + g + b + '_' +  xx.split('/')[-1]\n",
        "            io.imsave(file_name, image_rgb_flipped)\n",
        "\n",
        "        # Color Jitter\n",
        "        for jj in range(5):\n",
        "            image_jittered = color_jitter(image, brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05)\n",
        "            # print(image_jittered.shape)\n",
        "            file_name = '/'.join(xx.split('/')[:-1]) + '/' + 'jitter' + str(jj) + '_' +  xx.split('/')[-1]\n",
        "            io.imsave(file_name, image_jittered)\n",
        "\n",
        "\n",
        "def balance_classes(images_path):\n",
        "\n",
        "    '''\n",
        "        Balances classes with randomly cropped images.\n",
        "    '''\n",
        "\n",
        "    # print('\\nBalance Classes')\n",
        "\n",
        "    # Balance classes with random crop (only augment classes with lesser examples to create even distribution across all classes)\n",
        "    # (Both in the source and target domain)\n",
        "\n",
        "    classdict = count_class_distribution(images_path)\n",
        "    maxCount = np.max([len(classdict[c]) for c in classdict])\n",
        "    print('Initial Class distribution', [len(classdict[c]) for c in classdict])\n",
        "\n",
        "    writtenImagesCount = {c:0 for c in classdict}\n",
        "\n",
        "    for c in tqdm(classdict, desc=\"Balancing\", ncols=100):\n",
        "        n_c = len(classdict[c])\n",
        "\n",
        "        for i in range(maxCount - n_c):\n",
        "            randim = classdict[c][np.random.randint(len(classdict[c]))]\n",
        "            cropnum = 1\n",
        "            file_name = '/'.join(randim.split('/')[:-1]) + '/' + 'randcrop_' + str(cropnum) + '_' +  randim.split('/')[-1]\n",
        "            while(True):\n",
        "                if os.path.exists(file_name):\n",
        "                    cropnum += 1\n",
        "                    file_name = '/'.join(randim.split('/')[:-1]) + '/' + 'randcrop_' + str(cropnum) + '_' +  randim.split('/')[-1]\n",
        "                else:\n",
        "                    image = io.imread(randim)\n",
        "                    image_randcrop = random_crop(image, 224, 20)\n",
        "                    io.imsave(file_name, image_randcrop)\n",
        "                    writtenImagesCount[c] += 1\n",
        "                    break\n",
        "\n",
        "    print('Added Images', [writtenImagesCount[c] for c in writtenImagesCount])\n",
        "\n",
        "\n",
        "def make_augmentation_dictionary(index_list_path):\n",
        "\n",
        "    '''\n",
        "        Creates a dictionary with image filenames, and the list of their augmented images filenames.\n",
        "    '''\n",
        "\n",
        "    def get_augmentations(image_filename):\n",
        "\n",
        "        '''\n",
        "            Gets the names of the augmentation files.\n",
        "        '''\n",
        "\n",
        "        xx = image_filename\n",
        "\n",
        "        aug_files = []\n",
        "\n",
        "        # Flip image\n",
        "        file_name = '/'.join(xx.split('/')[:-1]) + '/' + 'flip_' +  xx.split('/')[-1]\n",
        "        aug_files.append(file_name)\n",
        "\n",
        "        # Rotate Image\n",
        "        for yy in [-3, -2, -1, 1, 2, 3]:\n",
        "            file_name = '/'.join(xx.split('/')[:-1]) + '/' + 'rotate' + str(yy) + '_' +  xx.split('/')[-1]\n",
        "            aug_files.append(file_name)\n",
        "\n",
        "        # RGB flip\n",
        "        for order in [(0, 2, 1), (1, 0, 2), (1, 2, 0), (2, 0, 1), (2, 1, 0)]:\n",
        "            r = str(order[0])\n",
        "            g = str(order[1])\n",
        "            b = str(order[2])\n",
        "            file_name = '/'.join(xx.split('/')[:-1]) + '/' + 'rgbflip' + r + g + b + '_' +  xx.split('/')[-1]\n",
        "            aug_files.append(file_name)\n",
        "\n",
        "        # Color Jitter\n",
        "        for jj in range(5):\n",
        "            file_name = '/'.join(xx.split('/')[:-1]) + '/' + 'jitter' + str(jj) + '_' +  xx.split('/')[-1]\n",
        "            aug_files.append(file_name)\n",
        "\n",
        "        return aug_files\n",
        "\n",
        "    def get_augmentations_prefix():\n",
        "\n",
        "        '''\n",
        "            Returns the list of prefixes for augmentation images.\n",
        "        '''\n",
        "\n",
        "        aug_prefix = []\n",
        "\n",
        "        # Flip image\n",
        "        aug_prefix.append('flip')\n",
        "\n",
        "        # Rotate Image\n",
        "        for yy in [-3, -2, -1, 1, 2, 3]:\n",
        "            aug_prefix.append('rotate' + str(yy))\n",
        "\n",
        "        # RGB flip\n",
        "        for order in [(0, 2, 1), (1, 0, 2), (1, 2, 0), (2, 0, 1), (2, 1, 0)]:\n",
        "            r = str(order[0]); g = str(order[1]); b = str(order[2])\n",
        "            aug_prefix.append('rgbflip' + r + g + b)\n",
        "\n",
        "        # Color Jitter\n",
        "        for jj in range(5):\n",
        "            aug_prefix.append('jitter' + str(jj))\n",
        "\n",
        "        # Randcrop\n",
        "        aug_prefix.append('randcrop')\n",
        "\n",
        "        return aug_prefix\n",
        "\n",
        "    all_fils = np.load(index_list_path)\n",
        "    pfix = get_augmentations_prefix()\n",
        "\n",
        "    only_images = [a for a in all_fils if a.split('/')[-1].split('_')[0] not in pfix]\n",
        "    print(len(only_images))\n",
        "\n",
        "    print('sanity check 1')\n",
        "    for f in tqdm(only_images):\n",
        "        assert f.split('/')[-1].split('_')[0]=='category'\n",
        "    print('correct')\n",
        "\n",
        "    dictionary = {}\n",
        "\n",
        "    print('sanity check 2')\n",
        "    for f in tqdm(only_images):\n",
        "        augfiles = get_augmentations(f)\n",
        "        dictionary[f] = augfiles\n",
        "        for fil in augfiles:\n",
        "            assert fil in all_fils\n",
        "    print('correct')\n",
        "\n",
        "    return dictionary"
      ],
      "metadata": {
        "id": "V3RihF_tczFq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "dataset_name = 'Office-31'  # 'office_31_dataset'\n",
        "experiments = ['usfda_office_31_DtoA']\n",
        "\n",
        "for dataset_exp_name in experiments:\n",
        "\n",
        "    # Rotate and augment source images\n",
        "    print('Rotation augmentation on source images')\n",
        "    os.system('rm -rf /content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/source_images/train/flip*.png')\n",
        "    os.system('rm -rf /content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/source_images/train/randcrop*.png')\n",
        "    os.system('rm -rf /content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/source_images/train/rotate*.png')\n",
        "    os.system('rm -rf /content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/source_images/train/rgbflip*.png')\n",
        "    os.system('rm -rf /content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/source_images/train/jitter*.png')\n",
        "\n",
        "    files = glob.glob('/content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/source_images/train/*.png')\n",
        "\n",
        "    # Wrap the entire file list with tqdm to track progress for all files\n",
        "    for file in tqdm(files, desc=\"Augmenting source images\", ncols=100):\n",
        "        augment_images([file])  # Process each file\n",
        "\n",
        "    # Balance source classes\n",
        "    print('Class balancing on source images')\n",
        "    balance_classes(files)\n",
        "\n",
        "    # Rotate and augment target images\n",
        "    print('Rotation augmentation on target images')\n",
        "    os.system('rm -rf /content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/target_images/train/flip*.png')\n",
        "    os.system('rm -rf /content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/target_images/train/randcrop*.png')\n",
        "    os.system('rm -rf /content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/target_images/train/rotate*.png')\n",
        "    os.system('rm -rf /content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/target_images/train/rgbflip*.png')\n",
        "    os.system('rm -rf /content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/target_images/train/jitter*.png')\n",
        "\n",
        "    files = glob.glob('/content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/target_images/train/*.png')\n",
        "    for file in tqdm(files, desc=\"Augmenting target images\", ncols=100):\n",
        "        augment_images([file])\n",
        "\n",
        "    # Balance target classes\n",
        "    print('Class balancing on target images')\n",
        "    balance_classes(files)\n",
        "\n",
        "    # # Save index list source images\n",
        "    # temp_paths_train = os.listdir('/content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/source_images/train/')\n",
        "    # temp_paths_train = ['/content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/source_images/train/' + x for x in temp_paths_train]\n",
        "    # save_path = '/content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/index_lists/source_images_index_list_train.npy'\n",
        "    # np.save(save_path , np.array(temp_paths_train))\n",
        "\n",
        "    # # Save index list target images\n",
        "    # temp_paths_train = os.listdir('/content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/target_images/train/')\n",
        "    # temp_paths_train = ['/content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/target_images/train/' + x for x in temp_paths_train]\n",
        "    # save_path = '/content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/index_lists/target_images_index_list_train.npy'\n",
        "    # np.save(save_path , np.array(temp_paths_train))\n",
        "\n",
        "    # # Make augmentation dictionary for source\n",
        "    # source_list_filename = '/content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/index_lists/source_images_index_list_train.npy'\n",
        "    # source_augs = make_augmentation_dictionary(source_list_filename)\n",
        "    # source_aug_dict_path = '/content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/index_lists/source_images_aug_dict_train.npy'\n",
        "    # np.save(source_aug_dict_path, source_augs)\n",
        "\n",
        "    # # Make augmentation dictionary for target\n",
        "    # target_list_filename = '/content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/index_lists/target_images_index_list_train.npy'\n",
        "    # target_augs = make_augmentation_dictionary(target_list_filename)\n",
        "    # target_aug_dict_path = '/content/drive/MyDrive/' + dataset_name + '/' + dataset_exp_name + '/index_lists/target_images_aug_dict_train.npy'\n",
        "    # np.save(target_aug_dict_path, target_augs)\n",
        "\n",
        "    # Define the save path for both source and target images\n",
        "    save_path = os.path.join('/content/drive/MyDrive', dataset_name, dataset_exp_name, 'index_lists')\n",
        "\n",
        "    # Save index list for source images\n",
        "    source_images_path = os.listdir(os.path.join('/content/drive/MyDrive', dataset_name, dataset_exp_name, 'source_images', 'train'))\n",
        "    source_images_path = [os.path.join('/content/drive/MyDrive', dataset_name, dataset_exp_name, 'source_images', 'train', x) for x in source_images_path]\n",
        "    np.save(os.path.join(save_path, 'source_images_index_list_train.npy'), source_images_path)\n",
        "\n",
        "    # Save index list for target images\n",
        "    target_images_path = os.listdir(os.path.join('/content/drive/MyDrive', dataset_name, dataset_exp_name, 'target_images', 'train'))\n",
        "    target_images_path = [os.path.join('/content/drive/MyDrive', dataset_name, dataset_exp_name, 'target_images', 'train', x) for x in target_images_path]\n",
        "    np.save(os.path.join(save_path, 'target_images_index_list_train.npy'), target_images_path)\n",
        "\n",
        "    # Make augmentation dictionary for source\n",
        "    source_list_filename = os.path.join('/content/drive/MyDrive', dataset_name, dataset_exp_name, 'index_lists', 'source_images_index_list_train.npy')\n",
        "    source_augs = make_augmentation_dictionary(source_list_filename)\n",
        "    source_aug_dict_path = os.path.join('/content/drive/MyDrive', dataset_name, dataset_exp_name, 'index_lists', 'source_images_aug_dict_train.npy')\n",
        "    np.save(source_aug_dict_path, source_augs)\n",
        "\n",
        "    # Make augmentation dictionary for target\n",
        "    target_list_filename = os.path.join('/content/drive/MyDrive', dataset_name, dataset_exp_name, 'index_lists', 'target_images_index_list_train.npy')\n",
        "    target_augs = make_augmentation_dictionary(target_list_filename)\n",
        "    target_aug_dict_path = os.path.join('/content/drive/MyDrive', dataset_name, dataset_exp_name, 'index_lists', 'target_images_aug_dict_train.npy')\n",
        "    np.save(target_aug_dict_path, target_augs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt7fBBmtb5Py",
        "outputId": "14fa7c75-e3a9-44ff-b913-e1906375b3fb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rotation augmentation on source images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Augmenting source images: 100%|███████████████████████████████████| 279/279 [03:04<00:00,  1.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class balancing on source images\n",
            "Initial Class distribution [10, 10, 9, 19, 10, 7, 18, 21, 11, 20, 21, 10, 14, 11, 12, 13, 13, 14, 27, 9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Balancing: 100%|████████████████████████████████████████████████████| 20/20 [00:09<00:00,  2.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added Images [17, 17, 18, 8, 17, 20, 9, 6, 16, 7, 6, 17, 13, 16, 15, 14, 14, 13, 0, 18]\n",
            "Rotation augmentation on target images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Augmenting target images:   1%|▎                                  | 20/1967 [00:12<20:56,  1.55it/s]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter1_category_ring_binder_category_number_14_dataset_amazon_1422.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images:   2%|▌                                  | 31/1967 [00:20<20:22,  1.58it/s]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter0_category_ring_binder_category_number_14_dataset_amazon_1433.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images:   2%|▊                                  | 44/1967 [00:28<22:36,  1.42it/s]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter2_category_ruler_category_number_15_dataset_amazon_1446.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter3_category_ruler_category_number_15_dataset_amazon_1446.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images:   3%|█                                  | 57/1967 [00:36<19:29,  1.63it/s]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter2_category_ruler_category_number_15_dataset_amazon_1459.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images:   3%|█▏                                 | 68/1967 [00:45<21:58,  1.44it/s]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter0_category_ruler_category_number_15_dataset_amazon_1470.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter1_category_ruler_category_number_15_dataset_amazon_1470.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter3_category_ruler_category_number_15_dataset_amazon_1470.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter4_category_ruler_category_number_15_dataset_amazon_1470.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images:   4%|█▍                                 | 83/1967 [00:54<20:42,  1.52it/s]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter0_category_ruler_category_number_15_dataset_amazon_1485.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images:   4%|█▍                                 | 84/1967 [00:55<20:22,  1.54it/s]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter2_category_ruler_category_number_15_dataset_amazon_1486.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter3_category_ruler_category_number_15_dataset_amazon_1486.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter4_category_ruler_category_number_15_dataset_amazon_1486.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images:   7%|██                              | 129/1967 [02:13<7:51:58, 15.41s/it]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/flip_category_scissors_category_number_16_dataset_amazon_1531.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/rotate-1_category_scissors_category_number_16_dataset_amazon_1531.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/rotate1_category_scissors_category_number_16_dataset_amazon_1531.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/rgbflip021_category_scissors_category_number_16_dataset_amazon_1531.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/rgbflip102_category_scissors_category_number_16_dataset_amazon_1531.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/rgbflip120_category_scissors_category_number_16_dataset_amazon_1531.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/rgbflip201_category_scissors_category_number_16_dataset_amazon_1531.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/rgbflip210_category_scissors_category_number_16_dataset_amazon_1531.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter0_category_scissors_category_number_16_dataset_amazon_1531.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter1_category_scissors_category_number_16_dataset_amazon_1531.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter2_category_scissors_category_number_16_dataset_amazon_1531.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter3_category_scissors_category_number_16_dataset_amazon_1531.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter4_category_scissors_category_number_16_dataset_amazon_1531.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images:  24%|████████                          | 465/1967 [04:42<10:20,  2.42it/s]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter1_category_tape_dispenser_category_number_19_dataset_amazon_1867.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter4_category_tape_dispenser_category_number_19_dataset_amazon_1867.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images:  33%|███████████▏                      | 646/1967 [05:59<10:11,  2.16it/s]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter0_category_mouse_category_number_4_dataset_amazon_483.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter4_category_mouse_category_number_4_dataset_amazon_483.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images:  34%|███████████▍                      | 659/1967 [06:05<09:46,  2.23it/s]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter1_category_mug_category_number_5_dataset_amazon_496.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter4_category_mug_category_number_5_dataset_amazon_496.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images:  34%|███████████▍                      | 662/1967 [06:06<07:49,  2.78it/s]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter0_category_mug_category_number_5_dataset_amazon_499.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images:  35%|███████████▉                      | 692/1967 [06:19<08:56,  2.38it/s]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter0_category_mug_category_number_5_dataset_amazon_529.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images:  36%|████████████▏                     | 708/1967 [06:27<11:24,  1.84it/s]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter0_category_mug_category_number_5_dataset_amazon_545.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter2_category_mug_category_number_5_dataset_amazon_545.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter4_category_mug_category_number_5_dataset_amazon_545.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images:  37%|████████████▋                     | 737/1967 [06:39<09:21,  2.19it/s]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter2_category_mug_category_number_5_dataset_amazon_574.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter4_category_mug_category_number_5_dataset_amazon_574.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images:  73%|████████████████████████         | 1435/1967 [11:26<04:40,  1.90it/s]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/rotate-3_category_punchers_category_number_13_dataset_amazon_1272.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/rotate3_category_punchers_category_number_13_dataset_amazon_1272.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images:  77%|█████████████████████████▍       | 1517/1967 [12:01<02:48,  2.67it/s]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter0_category_ring_binder_category_number_14_dataset_amazon_1354.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter1_category_ring_binder_category_number_14_dataset_amazon_1354.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter2_category_ring_binder_category_number_14_dataset_amazon_1354.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter4_category_ring_binder_category_number_14_dataset_amazon_1354.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images:  78%|█████████████████████████▊       | 1536/1967 [12:09<03:05,  2.32it/s]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter3_category_ring_binder_category_number_14_dataset_amazon_1373.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images:  78%|█████████████████████████▊       | 1538/1967 [12:10<02:56,  2.43it/s]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter3_category_ring_binder_category_number_14_dataset_amazon_1375.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images:  79%|██████████████████████████▏      | 1560/1967 [12:20<03:18,  2.05it/s]/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py:328: UserWarning: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/target_images/train/jitter4_category_ring_binder_category_number_14_dataset_amazon_1397.png is a low contrast image\n",
            "  return func(*args, **kwargs)\n",
            "Augmenting target images: 100%|█████████████████████████████████| 1967/1967 [15:20<00:00,  2.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class balancing on target images\n",
            "Initial Class distribution [90, 75, 100, 99, 99, 96, 64, 100, 94, 82, 100, 99, 98, 95, 93, 100, 98, 92, 94, 100, 99]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Balancing: 100%|████████████████████████████████████████████████████| 21/21 [00:03<00:00,  5.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added Images [10, 25, 0, 1, 1, 4, 36, 0, 6, 18, 0, 1, 2, 5, 7, 0, 2, 8, 6, 0, 1]\n",
            "279\n",
            "sanity check 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 279/279 [00:00<00:00, 104334.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct\n",
            "sanity check 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 279/279 [00:03<00:00, 91.70it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct\n",
            "1967\n",
            "sanity check 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1967/1967 [00:00<00:00, 350133.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct\n",
            "sanity check 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1967/1967 [01:31<00:00, 21.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1jpTZUE1qOfy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "127e920c-ad64-445d-b79b-de76531ad011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "{'running_supervised': True, 'start_iter': 1, 'max_iter': 10, 'val_after': 5, 'C': ['back_pack', 'calculator', 'keyboard', 'monitor', 'mouse', 'mug', 'bike', 'laptop_computer', 'headphones', 'projector'], 'Cs_dash': ['bike_helmet', 'bookcase', 'bottle', 'desk_chair', 'desk_lamp', 'desktop_computer', 'file_cabinet', 'letter_tray', 'mobile_phone', 'paper_notebook'], 'Ct_dash': ['pen', 'phone', 'printer', 'punchers', 'ring_binder', 'ruler', 'scissors', 'speaker', 'stapler', 'tape_dispenser', 'trash_can'], 'num_C': 10, 'num_Cs_dash': 10, 'num_Ct_dash': 11, 'num_Cs': 20, 'num_Ct': 21, 'batch_size': 64, 'num_positive_samples': 32, 'num_negative_samples': 32, 'num_positive_images': 64, 'num_negative_images': 64, 'cnn_to_use': 'resnet50', 'Fs_dims': 256, 'softmax_temperature': 1, 'online_augmentation_90_degrees': True, 'val_aug_imgs_mean_before_softmax': False, 'val_aug_imgs_mean_after_softmax': True, 'load_weights': False, 'load_exp_name': 'None', 'exp_name': 'usfda_office_31_DtoA', 'optimizer': {'classification': ['M', 'Fs', 'Cs', 'Cn'], 'pos_img_recon': ['Fs', 'G'], 'pos_sample_recon': ['Fs', 'G'], 'logsoftmax': ['Fs']}, 'use_loss': {'classification': True, 'pos_img_recon': True, 'pos_sample_recon': True, 'logsoftmax': True}, 'losses_after_enough_iters': ['logprob', 'logsoftmax', 'pos_sample_recon'], 'classification_weight': [1, 0.2], 'to_train': {'M': False, 'Fs': True, 'Ft': False, 'G': True, 'Cs': True, 'Cn': True}, 'lr': 0.0001, 'device': device(type='cuda'), 'gpu': 0, 'dataset_exp_name': 'usfda_office_31_DtoA', 'weights_path': '/content/drive/MyDrive/Office-31/usfda_office_31_DtoA/weights', 'summaries_path': '/content/drive/MyDrive/Office-31/usfda_office_31_DtoA/summaries', 'dataset_path': '/content/drive/MyDrive/Office-31/usfda_office_31_DtoA/index_lists', 'negative_data_path': '/content/drive/MyDrive/Office-31/usfda_office_31_DtoA/negative_images', 'negative_mask_path': '/content/drive/MyDrive/Office-31/usfda_office_31_DtoA/negative_masks'}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from glob import glob\n",
        "from natsort import natsorted\n",
        "\n",
        "# Configuration Settings\n",
        "settings = {}\n",
        "\n",
        "# Supervised Training Toggle\n",
        "settings['running_supervised'] = True\n",
        "\n",
        "# Training Parameters\n",
        "settings['start_iter'] = 1\n",
        "settings['max_iter'] = 10\n",
        "settings['val_after'] = 5\n",
        "\n",
        "# Label Set Relationships\n",
        "settings['C'] = ['back_pack', 'calculator', 'keyboard', 'monitor', 'mouse', 'mug', 'bike', 'laptop_computer', 'headphones', 'projector']\n",
        "settings['Cs_dash'] = ['bike_helmet', 'bookcase', 'bottle', 'desk_chair', 'desk_lamp', 'desktop_computer', 'file_cabinet', 'letter_tray', 'mobile_phone', 'paper_notebook']\n",
        "settings['Ct_dash'] = ['pen', 'phone', 'printer', 'punchers', 'ring_binder', 'ruler', 'scissors', 'speaker', 'stapler', 'tape_dispenser', 'trash_can']\n",
        "\n",
        "settings['num_C'] = len(settings['C'])\n",
        "settings['num_Cs_dash'] = len(settings['Cs_dash'])\n",
        "settings['num_Ct_dash'] = len(settings['Ct_dash'])\n",
        "settings['num_Cs'] = settings['num_C'] + settings['num_Cs_dash']\n",
        "settings['num_Ct'] = settings['num_C'] + settings['num_Ct_dash']\n",
        "\n",
        "# Batch Sizes and Sample Counts\n",
        "settings['batch_size'] = 64\n",
        "settings['num_positive_samples'] = 32\n",
        "settings['num_negative_samples'] = 32\n",
        "settings['num_positive_images'] = settings['batch_size']\n",
        "settings['num_negative_images'] = settings['batch_size']\n",
        "\n",
        "# Model Parameters\n",
        "settings['cnn_to_use'] = 'resnet50'\n",
        "settings['Fs_dims'] = 256\n",
        "settings['softmax_temperature'] = 1\n",
        "settings['online_augmentation_90_degrees'] = True  # Augmentation during training\n",
        "settings['val_aug_imgs_mean_before_softmax'] = False\n",
        "settings['val_aug_imgs_mean_after_softmax'] = True\n",
        "\n",
        "# Weights and Experiment Name\n",
        "settings['load_weights'] = False\n",
        "settings['load_exp_name'] = 'None'\n",
        "settings['exp_name'] = 'usfda_office_31_DtoA'\n",
        "\n",
        "# Optimizers for Losses\n",
        "settings['optimizer'] = {\n",
        "    'classification': ['M', 'Fs', 'Cs', 'Cn'],\n",
        "    'pos_img_recon': ['Fs', 'G'],\n",
        "    'pos_sample_recon': ['Fs', 'G'],\n",
        "    'logsoftmax': ['Fs'],\n",
        "}\n",
        "\n",
        "# Loss Usage\n",
        "settings['use_loss'] = {\n",
        "    'classification': True,\n",
        "    'pos_img_recon': True,\n",
        "    'pos_sample_recon': True,\n",
        "    'logsoftmax': True,\n",
        "}\n",
        "\n",
        "settings['losses_after_enough_iters'] = ['logprob', 'logsoftmax', 'pos_sample_recon']\n",
        "settings['classification_weight'] = [1, 0.2]  # Hyperparameter alpha\n",
        "\n",
        "# Model Training Toggles\n",
        "settings['to_train'] = {\n",
        "    'M': False,  # Frozen ResNet-50\n",
        "    'Fs': True,\n",
        "    'Ft': False,\n",
        "    'G': True,\n",
        "    'Cs': True,\n",
        "    'Cn': True,\n",
        "}\n",
        "\n",
        "# Learning Rate and Device Configuration\n",
        "# Learning Rate and Device Configuration\n",
        "settings['lr'] = 1e-4\n",
        "settings['device'] = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    settings['gpu'] = torch.cuda.current_device()\n",
        "    torch.cuda.set_device(settings['gpu'])\n",
        "else:\n",
        "    settings['gpu'] = None  # Set to None when using CPU\n",
        "\n",
        "# Print device information for verification\n",
        "print(f\"Using device: {settings['device']}\")\n",
        "\n",
        "settings['dataset_exp_name'] = 'usfda_office_31_DtoA'\n",
        "\n",
        "settings['weights_path'] = os.path.join(server_root_path, 'Office-31', settings['dataset_exp_name'], 'weights')\n",
        "settings['summaries_path'] = os.path.join(server_root_path, 'Office-31', settings['dataset_exp_name'], 'summaries')\n",
        "\n",
        "# Loading Pretrained Weights\n",
        "if settings['load_weights']:\n",
        "    best_weights = natsorted(glob(os.path.join(settings['weights_path'], settings['load_exp_name'], '*.pth')))[-1]\n",
        "    settings['load_weights_path'] = best_weights\n",
        "\n",
        "# Dataset and Paths\n",
        "settings['dataset_path'] = os.path.join(server_root_path, 'Office-31', settings['dataset_exp_name'], 'index_lists')\n",
        "settings['negative_data_path'] = os.path.join(server_root_path, 'Office-31', settings['dataset_exp_name'], 'negative_images')\n",
        "settings['negative_mask_path'] = os.path.join(server_root_path, 'Office-31', settings['dataset_exp_name'], 'negative_masks')\n",
        "\n",
        "# Print settings to verify\n",
        "print(settings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import pdb\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "\n",
        "Fs_dims = settings['Fs_dims']\n",
        "cnn_to_use = settings['cnn_to_use']\n",
        "\n",
        "class CustomResNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(CustomResNet, self).__init__()\n",
        "\n",
        "        temp_resnet = models.resnet50(pretrained=True)\n",
        "        self.features = nn.Sequential(*[x for x in list(temp_resnet.children())[:-1]]) # Upto the avgpool layer\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        feats = self.features(x)\n",
        "        return feats.view((x.shape[0], 2048))\n",
        "\n",
        "\n",
        "class modnet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_C, num_Cs_dash, num_Ct_dash, cnn=cnn_to_use, additional_components=[]):\n",
        "\n",
        "        super(modnet, self).__init__()\n",
        "\n",
        "        # Frozen initial conv layers\n",
        "        if cnn=='resnet50':\n",
        "            self.M = CustomResNet()\n",
        "        else:\n",
        "            raise NotImplementedError('Not implemented for ' + str(cnn))\n",
        "\n",
        "        self.Fs = nn.Sequential(\n",
        "            nn.Linear(2048,1024),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024,1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024,Fs_dims),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(Fs_dims, Fs_dims),\n",
        "            nn.BatchNorm1d(Fs_dims),\n",
        "            nn.ELU(),\n",
        "        )\n",
        "\n",
        "        self.Ft = nn.Sequential(\n",
        "            nn.Linear(2048,1024),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024,1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024,Fs_dims),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(Fs_dims, Fs_dims),\n",
        "            nn.BatchNorm1d(Fs_dims),\n",
        "            nn.ELU(),\n",
        "        )\n",
        "\n",
        "        self.G = nn.Sequential(\n",
        "            nn.Linear(Fs_dims,1024),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024,1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024,2048),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(2048, 2048),\n",
        "        )\n",
        "\n",
        "        self.Cs = nn.Sequential(\n",
        "            nn.Linear(Fs_dims, num_C + num_Cs_dash)\n",
        "        )\n",
        "\n",
        "        # Negative class classifier. Change this to vary the size of the negative class classifier.\n",
        "        n = settings['num_C'] + settings['num_Cs_dash']\n",
        "        num_negative_classes = int(n*(n-1)/2)\n",
        "        # num_negative_classes = 150\n",
        "\n",
        "        self.Cn = nn.Sequential(\n",
        "            nn.Linear(Fs_dims, num_negative_classes)\n",
        "        )\n",
        "\n",
        "        self.components = {\n",
        "            'M': self.M,\n",
        "            'Fs': self.Fs,\n",
        "            'Ft': self.Ft,\n",
        "            'G': self.G,\n",
        "            'Cs': self.Cs,\n",
        "            'Cn': self.Cn,\n",
        "        }\n",
        "\n",
        "    def forward(self, x, which_fext='original'):\n",
        "        raise NotImplementedError('Implemented a custom forward in train loop')\n",
        "\n",
        "\n",
        "def no_param(model):\n",
        "    param = 0\n",
        "    for p in list(model.parameters()):\n",
        "        n=1\n",
        "        for i in list(p.size()):\n",
        "            n*= i\n",
        "        param += n\n",
        "    return param\n",
        "\n",
        "\n",
        "# if __name__=='__main__':\n",
        "#     raise NotImplementedError('Please check README.md for execution details')"
      ],
      "metadata": {
        "id": "LQPIpbiMAdJa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage import io, transform\n",
        "from torchvision import transforms, utils\n",
        "import torch\n",
        "import os\n",
        "\n",
        "class TemplateDataset(Dataset):\n",
        "\n",
        "    def __init__(self, view_params_set, transform=None, random_choice=True, online_augmentation_90_degrees=None):\n",
        "\n",
        "        self.view_params_set = view_params_set\n",
        "        self.transform = transform\n",
        "        self.data = {}\n",
        "        self.random_choice = random_choice\n",
        "        self.online_augmentation_90_degrees = online_augmentation_90_degrees\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.random_choice:\n",
        "            return 1000000\n",
        "        else:\n",
        "            return len(self.view_params_set)\n",
        "\n",
        "    def get_random_90_degree_augmentation(self, img):\n",
        "        assert (img.shape == (224, 224, 3)) or (img.shape == (32, 32, 3))\n",
        "\n",
        "        if type(img) == np.ndarray:\n",
        "            img = transforms.ToPILImage()(img)\n",
        "\n",
        "        angle = np.random.choice([-90, 90])\n",
        "        angle = int(angle)\n",
        "        new_img = transforms.functional.rotate(img, angle)\n",
        "\n",
        "        return new_img\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if self.random_choice:\n",
        "            idx = np.random.choice(np.arange(len(self.view_params_set)), 1)[0]\n",
        "        img_name = os.path.join(server_root_path, self.view_params_set[idx])\n",
        "        image = io.imread(img_name)\n",
        "        image_cp = image\n",
        "        if self.online_augmentation_90_degrees is None:\n",
        "            if settings['online_augmentation_90_degrees']:\n",
        "                image_cp = self.get_random_90_degree_augmentation(image_cp)\n",
        "        else:\n",
        "            if self.online_augmentation_90_degrees:\n",
        "                image_cp = self.get_random_90_degree_augmentation(image_cp)\n",
        "        res = transforms.Compose([transforms.ToTensor()])\n",
        "        image_cp = res(image_cp)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        norm = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "        image = norm(image)\n",
        "        self.data['img'] = torch.tensor(image)\n",
        "        # print img_name\n",
        "        # self.data['label'] = min(config.num_classes_known, int(img_name.split('_')[-4]))\n",
        "        self.data['label'] = int(img_name.split('_')[-4])\n",
        "        # print 'label', self.data['label']\n",
        "        self.data['raw'] = image_cp\n",
        "        self.data['filename'] = img_name\n",
        "\n",
        "        return self.data.copy()"
      ],
      "metadata": {
        "id": "HhabCPV9AuSw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from natsort import natsorted\n",
        "\n",
        "def PCA(data_pca, k):\n",
        "    data_mean = torch.mean(data_pca,0)\n",
        "    data_pca = data_pca - data_mean.expand_as(data_pca)\n",
        "    U,S,V = torch.svd(torch.t(data_pca))\n",
        "    Components = torch.mm(data_pca,U[:,:k])\n",
        "    return Components\n",
        "\n",
        "def Get_Full_Data(loader, net_G, which_fext = 'original', vgg_feats = True, which_level_features='default', use_numpy=False):\n",
        "\n",
        "    label_list = []\n",
        "    feature_list = []\n",
        "    common_embedding_list = []\n",
        "    prediction_list = []\n",
        "    vggrec_list = []\n",
        "    vggfeats_list = []\n",
        "\n",
        "    L = len(loader)\n",
        "    loader = enumerate(loader)\n",
        "    i = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        while(True):\n",
        "            try:\n",
        "                data = loader.next()[1]\n",
        "                i+=1\n",
        "                if i%10==0:\n",
        "                    print(str(i) + ' / ' + str(L))\n",
        "                # if i>320:\n",
        "                #     break\n",
        "            except:\n",
        "                break\n",
        "\n",
        "            x = Variable(data['img'][:, :3, :, :]).to(settings['device']).float()\n",
        "\n",
        "            if which_level_features=='default':\n",
        "                prediction, feature, vggrec, vggfeats = net_G.forward(x, which_fext=which_fext)\n",
        "            else:\n",
        "                prediction, feature, vggrec, vggfeats = net_G.forward_and_get_features(x, which_fext=which_fext, which_level_features=which_level_features)\n",
        "                feature = feature.cpu()\n",
        "\n",
        "            if vgg_feats:\n",
        "                vggrec_list.append(vggrec.detach())\n",
        "                vggfeats_list.append(vggfeats.detach())\n",
        "            feature_list.append(feature.detach())\n",
        "            prediction_list.append(prediction.detach())\n",
        "            label = Variable(torch.LongTensor(data['label'])).to(settings['device']).float()\n",
        "            label_list.append(label.detach())\n",
        "\n",
        "        if use_numpy:\n",
        "            data_x = np.concatenate(feature_list, 0)\n",
        "            data_y = np.concatenate(label_list, 0)\n",
        "            data_pred = np.concatenate(prediction_list, 0)\n",
        "        else:\n",
        "            data_x = torch.cat(feature_list, 0)\n",
        "            data_y = torch.cat(label_list, 0)\n",
        "            data_pred = torch.cat(prediction_list, 0)\n",
        "\n",
        "        if vgg_feats:\n",
        "            if use_numpy:\n",
        "                data_vggrec = np.concatenate(vggrec_list, 0)\n",
        "                data_vggfeats = np.concatenate(vggfeats_list, 0)\n",
        "            else:\n",
        "                data_vggrec = torch.cat(vggrec_list, 0)\n",
        "                data_vggfeats = torch.cat(vggfeats_list, 0)\n",
        "    if vgg_feats:\n",
        "        return data_x, data_y, data_pred, data_vggrec, data_vggfeats\n",
        "    else:\n",
        "        return data_x, data_y, data_pred, None, None\n",
        "\n",
        "\n",
        "def Get_Cov_Mean(features):\n",
        "    # Features = (D, N)\n",
        "    features_mean = torch.mean(features,dim=1)\n",
        "    mean_cen_feat = features - features_mean[:, None]\n",
        "    COV = 1.0/(mean_cen_feat.size(1)-1) * mean_cen_feat.mm(mean_cen_feat.t())\n",
        "    MEAN = features_mean\n",
        "    eps = 1e-3 * torch.eye(COV.shape[0]).to(settings['device'])\n",
        "    return COV+eps, MEAN\n",
        "\n",
        "def get_mu_sig(feature_trans, labels, num_classes, only_return = False):\n",
        "    cov = []\n",
        "    std = []\n",
        "    mean = []\n",
        "    boundary = []\n",
        "    mean_value = []\n",
        "    eig_dir = []\n",
        "    COV, MEAN = Get_Cov_Mean(feature_trans.t())\n",
        "    # print feature_trans.t().shape\n",
        "    u,s,v = torch.svd(COV)\n",
        "    eigen_direction_big_circle = torch.matmul(v, torch.sqrt(s))\n",
        "    # here range = number of categories. hence, take care to change accordingly\n",
        "    for i in tqdm(range(num_classes)):\n",
        "        new_ft = feature_trans[labels==i]\n",
        "        features = new_ft.t()\n",
        "        # print features.shape\n",
        "        tmp_cov, tmp_mean = Get_Cov_Mean(features)\n",
        "        std.append(torch.sqrt(torch.diag(tmp_cov)))\n",
        "        cov.append(tmp_cov)\n",
        "        mean.append(tmp_mean)\n",
        "        # print(tmp_cov)\n",
        "        # print([float(tmp_cov[i, i]) for i in range(tmp_cov.shape[0])])\n",
        "        newd = MultivariateNormal(tmp_mean, tmp_cov)\n",
        "        llh = newd.log_prob(tmp_mean)\n",
        "        mean_value.append(llh)\n",
        "        u,s,v = torch.svd(tmp_cov)\n",
        "        eigen_direction = torch.matmul(v, torch.sqrt(s))\n",
        "        eig_dir.append(eigen_direction)\n",
        "\n",
        "    data = {}\n",
        "    for i in range(num_classes):\n",
        "        # print mean[i], cov[i], eig_dir[i]\n",
        "        data[str(i)] = [mean[i].data.cpu().numpy(), cov[i].data.cpu().numpy(), eig_dir[i].data.cpu().numpy()]\n",
        "    data['full_circle'] = [MEAN.data.cpu().numpy(), COV.data.cpu().numpy(), eigen_direction_big_circle]\n",
        "\n",
        "    if only_return:\n",
        "        return data\n",
        "    else:\n",
        "        np.save('mu_sigmanew', data)\n",
        "        return data"
      ],
      "metadata": {
        "id": "IEdDZuCZA_xP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvgJMN79BrRc",
        "outputId": "0c9621c3-7fa0-4bbf-94d7-7ac8134bef4e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (4.25.5)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m92.2/101.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "from torchvision import transforms, utils\n",
        "from glob import glob\n",
        "import time\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "from matplotlib.figure import Figure\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from tensorboardX import SummaryWriter\n",
        "import pdb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import norm\n",
        "from scipy.interpolate import make_interp_spline, BSpline\n",
        "from skimage import io\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "class TrainerG():\n",
        "\n",
        "    def __init__(self, network, optimizer, exp_name, index_lists, settings):\n",
        "        # print('function __init__')\n",
        "\n",
        "        # Set the network and optimizer\n",
        "        self.network = network\n",
        "        self.to_train = settings['to_train']\n",
        "\n",
        "        # Optimizers to use\n",
        "        self.optimizer = optimizer\n",
        "        self.which_optimizer = list(sorted(self.optimizer.keys()))\n",
        "        print('\\noptimizers: ' + str(self.which_optimizer) + '\\n')\n",
        "\n",
        "        # Save the settings\n",
        "        self.settings = settings\n",
        "\n",
        "        # Initialize the val and train writers\n",
        "        self.val_writer = SummaryWriter(os.path.join(server_root_path, 'Office-31', exp_name, 'summaries', 'logdir_val'))\n",
        "        self.train_writer = SummaryWriter(os.path.join(server_root_path, 'Office-31', exp_name, 'summaries', 'logdir_train'))\n",
        "\n",
        "        # Extract commonly used settings\n",
        "        self.batch_size = settings['batch_size']\n",
        "        self.current_iteration = settings['start_iter']\n",
        "\n",
        "        # Get the index lists\n",
        "        [index_list_path_train_source, index_list_path_val_source, _, _, _, _] = index_lists\n",
        "        self.index_list_train_source = np.load(index_list_path_train_source)\n",
        "        self.index_list_val_source = np.load(index_list_path_val_source)\n",
        "        self.index_list_train_negative = glob(self.settings['negative_data_path'] + '/*.png')\n",
        "\n",
        "        # Get number of classes\n",
        "        self.num_C = settings['num_C']\n",
        "        self.num_Cs_dash = settings['num_Cs_dash']\n",
        "        self.num_Ct_dash = settings['num_Ct_dash']\n",
        "        self.num_Cs = settings['num_Cs']\n",
        "        self.num_Ct = settings['num_Ct']\n",
        "\n",
        "        # Initialize data loaders\n",
        "        self.get_all_dataloaders()\n",
        "\n",
        "        self.mu_sigma = None\n",
        "        self.mu_sigma_distribution = None\n",
        "\n",
        "        # Mu Sigma is calculated at each validation iteration. For the first iteration, no mu sigmas exist, so don't load unless start iteration is higher.\n",
        "        if (self.current_iteration >= self.settings['val_after']):\n",
        "\n",
        "            self.recalculate_mu_sigma()\n",
        "\n",
        "\n",
        "    def get_mu_sigma_threshold(self, mu_sigma, mu_sigma_distro, maximum=True):\n",
        "        # print('function get_mu_sigma_threshold')\n",
        "\n",
        "        MU, COV = torch.from_numpy(mu_sigma[0]).to(device), torch.from_numpy(mu_sigma[1]).to(device)\n",
        "        u,s,v = torch.svd(COV)\n",
        "        if maximum:\n",
        "            eigen_direction = v[:, 0]\n",
        "        else:\n",
        "            eigen_direction = torch.matmul(v, torch.sqrt(s))\n",
        "\n",
        "        threshvec = MU + 3 * eigen_direction\n",
        "        return mu_sigma_distro.log_prob(threshvec)\n",
        "\n",
        "\n",
        "    def get_histogram(self, features, bin_size=0.01, normalize=True):\n",
        "        # print('function get_histogram')\n",
        "\n",
        "        F = [int(float(f)/bin_size) for f in features]\n",
        "        D = {}\n",
        "        for f in F:\n",
        "            if f in D: D[f] += 1\n",
        "            else: D[f] = 1\n",
        "        x = np.array([float(k*bin_size) for k in sorted(D.keys())])\n",
        "        y = np.array([float(D[f]) for f in sorted(D.keys())])\n",
        "\n",
        "        if normalize:\n",
        "            y = y / np.sum(y)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "\n",
        "    def get_all_dataloaders(self):\n",
        "        # print('function get_all_dataloaders')\n",
        "\n",
        "        dataset_train = TemplateDataset(self.index_list_train_source, transform=transforms.Compose([transforms.ToTensor()]), random_choice=False)\n",
        "        self.loader_train = DataLoader(dataset_train, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "        dataset_source_val = TemplateDataset(self.index_list_val_source, transform=transforms.Compose([transforms.ToTensor()]), random_choice=False)\n",
        "        self.loader_source_val = DataLoader(dataset_source_val, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "        dataset_source_train_negative = TemplateDataset(self.index_list_train_negative, transform=transforms.Compose([transforms.ToTensor()]), random_choice=False)\n",
        "        self.loader_train_negative = DataLoader(dataset_source_train_negative, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "    def get_loss(self, which_loss):\n",
        "        # print('function get_loss')\n",
        "\n",
        "        if which_loss == 'classification':\n",
        "            outs = torch.cat([self.features['Cs'], self.features['Cn']], dim=-1)\n",
        "            topK = self.settings['num_positive_images']\n",
        "            positive_image_loss = nn.CrossEntropyLoss(reduction='mean')(outs[:topK], self.gt[:topK].long())\n",
        "            negative_image_loss = nn.CrossEntropyLoss(reduction='mean')(outs[topK:], self.gt[topK:].long())\n",
        "            w1, w2 = self.settings['classification_weight']\n",
        "            loss = w1 * positive_image_loss + w2 * negative_image_loss\n",
        "\n",
        "        elif which_loss == 'pos_img_recon':\n",
        "            topK = self.settings['num_positive_samples']\n",
        "            loss = torch.mean( torch.sum(torch.pow(self.features['M'][:topK] - self.features['G'][:topK], 2), dim=-1) )\n",
        "\n",
        "        elif which_loss == 'pos_sample_recon':\n",
        "            topK = self.settings['num_positive_samples']\n",
        "            loss = torch.mean( torch.sum( torch.pow(self.features['Fs_sample'][:topK] - self.features['Fs_sample_recon'][:topK], 2), dim=-1 ) )\n",
        "\n",
        "        elif which_loss == 'logsoftmax': # Increase the class confidence by decreasing the distance\n",
        "            # This is L_p defined in the paper. While implementing in PyTorch, we decrease the distance to the corresponding cluster,\n",
        "            # which has the same effect as increasing the relative posterior confidence.\n",
        "            topK = self.settings['num_positive_images']\n",
        "            malhanobis_squared_mat = torch.zeros((topK, self.num_C + self.num_Cs_dash)).to(device)\n",
        "            for c in range(self.num_C + self.num_Cs_dash):\n",
        "                mu = torch.from_numpy(self.mu_sigma[str(c)][0]).to(device)\n",
        "                malhanobis_squared_distance = (self.mu_sigma_distribution[str(c)].log_prob(mu) - self.mu_sigma_distribution[str(c)].log_prob(self.features['Fs'][:topK])) ** 2\n",
        "                malhanobis_squared_mat[:, c] = malhanobis_squared_distance\n",
        "            norm_malhanobis_mat = malhanobis_squared_mat / torch.sum(malhanobis_squared_mat, dim=-1, keepdim=True)\n",
        "            corr_conf = torch.zeros((self.settings['batch_size'],)).to(device)\n",
        "            for i in range(self.settings['batch_size']):\n",
        "                corr_conf[i] = norm_malhanobis_mat[i, self.gt[i]]\n",
        "            loss = torch.mean( torch.log(corr_conf), dim=0 )\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError('Not implemented loss function ' + str(which_loss))\n",
        "\n",
        "        self.summary_dict['loss/' + str(which_loss)] = loss.data.cpu().numpy()\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def recalculate_mu_sigma(self):\n",
        "        # print('function recalculate_mu_sigma')\n",
        "        print('\\nRecalculating mu sigma\\n')\n",
        "        self.mu_sigma = self.get_mu_sigma()\n",
        "        self.mu_sigma_distribution = {}\n",
        "\n",
        "        for key in self.mu_sigma:\n",
        "            mu, sigma = torch.from_numpy(self.mu_sigma[key][0]).to(device), torch.from_numpy(self.mu_sigma[key][1]).to(device)\n",
        "            self.mu_sigma_distribution[key] = torch.distributions.MultivariateNormal(mu, sigma)\n",
        "\n",
        "        self.mu_sigma_threshold = []\n",
        "        for key in range(self.num_C + self.num_Cs_dash):\n",
        "            self.mu_sigma_threshold.append( self.get_mu_sigma_threshold(self.mu_sigma[str(key)], self.mu_sigma_distribution[str(key)]) )\n",
        "        # print(self.mu_sigma_threshold)\n",
        "        self.mu_sigma_threshold = torch.FloatTensor(self.mu_sigma_threshold).view((1, self.num_C + self.num_Cs_dash)).to(device)\n",
        "\n",
        "\n",
        "    def loss(self):\n",
        "        # print('function loss')\n",
        "\n",
        "        # ==================================\n",
        "        # ====== Accuracy over images ======\n",
        "        # ==================================\n",
        "        concat_outputs_img = torch.cat([self.features['Cs'], self.features['Cn']], dim=-1)\n",
        "        concat_softmax_img = F.softmax(concat_outputs_img/self.settings['softmax_temperature'], dim=-1)\n",
        "        pred_classes_img = torch.argmax(concat_softmax_img, dim=-1)\n",
        "        pos = self.settings['num_positive_images']\n",
        "\n",
        "        # Source Accuracy (Positive images)\n",
        "        pred_classes_pos_img = pred_classes_img[:pos]\n",
        "        gt_classes_pos_img = self.gt[:pos]\n",
        "        if len(pred_classes_pos_img) != 0:\n",
        "            source_acc_pos_img = (pred_classes_pos_img.float() == gt_classes_pos_img.float()).float().mean()\n",
        "            self.summary_dict['acc/source_acc_pos_img'] = source_acc_pos_img\n",
        "\n",
        "        # Source Accuracy (Negative images)\n",
        "        pred_classes_neg_img = pred_classes_img[pos:]\n",
        "        gt_classes_neg_img = self.gt[pos:]\n",
        "        if len(pred_classes_neg_img) != 0:\n",
        "            source_acc_neg_img = (pred_classes_neg_img.float() == gt_classes_neg_img.float()).float().mean()\n",
        "            source_binary_acc_neg_img = (pred_classes_neg_img.float() >= self.num_Cs).float().mean()\n",
        "            self.summary_dict['acc/source_acc_neg_img'] = source_acc_neg_img\n",
        "            self.summary_dict['acc/source_binary_acc_neg_img'] = source_binary_acc_neg_img\n",
        "\n",
        "        # ===================================\n",
        "        # ====== Accuracy over samples ======\n",
        "        # ===================================\n",
        "        if self.current_iteration >= self.settings['val_after']:\n",
        "            concat_outputs_sample = torch.cat([self.features['Cs_sample'], self.features['Cn_sample']], dim=-1)\n",
        "            concat_softmax_sample = F.softmax(concat_outputs_sample/self.settings['softmax_temperature'], dim=-1)\n",
        "            pred_classes_sample = torch.argmax(concat_softmax_sample, dim=-1)\n",
        "            pred_classes_sample = pred_classes_sample.view((pred_classes_sample.shape[0],))\n",
        "            pos = self.settings['num_positive_samples']\n",
        "\n",
        "            # Sample Accuracy (Positive samples)\n",
        "            pred_classes_pos_samples = pred_classes_sample[:pos]\n",
        "            gt_classes_pos_samples = self.gt_sample[:pos]\n",
        "            source_acc_pos_samples = (pred_classes_pos_samples.float() == gt_classes_pos_samples.float()).float().mean()\n",
        "            self.summary_dict['acc/source_acc_pos_samples'] = source_acc_pos_samples\n",
        "\n",
        "            # # Sample Accuracy (Negative samples)\n",
        "            # pred_classes_neg_samples = pred_classes_sample[pos:]\n",
        "            # gt_classes_neg_samples = self.gt_sample[pos:]\n",
        "            # source_acc_neg_samples = (pred_classes_neg_samples.float() == gt_classes_neg_samples.float()).float().mean()\n",
        "            # self.summary_dict['acc/source_acc_neg_samples'] = source_acc_neg_samples\n",
        "\n",
        "        if self.phase == 'train':\n",
        "\n",
        "            # ====== BACKPROP LOSSES ======\n",
        "            enough_iters = (self.current_iteration >= self.settings['val_after'])\n",
        "            l = len(self.which_optimizer)\n",
        "            current_loss = self.which_optimizer[self.current_iteration%l]\n",
        "\n",
        "            if self.settings['use_loss'][current_loss] and self.backward:\n",
        "\n",
        "                print('\\nApplying loss ' + str(self.which_optimizer[self.current_iteration%l]))\n",
        "\n",
        "                if current_loss in self.settings['losses_after_enough_iters']:\n",
        "                    if self.current_iteration >= self.settings['val_after']:\n",
        "                        # print('{} >= {}'.format(self.current_iteration, self.settings['val_after']))\n",
        "                        self.optimizer[self.which_optimizer[self.current_iteration%l]].zero_grad()\n",
        "                        loss = self.get_loss(which_loss=self.which_optimizer[self.current_iteration%l])\n",
        "                        self.summary_dict['loss/' + str(self.which_optimizer[self.current_iteration%l])] = loss.cpu().detach().numpy()\n",
        "                        loss.backward()\n",
        "                        self.optimizer[self.which_optimizer[self.current_iteration%l]].step()\n",
        "                else:\n",
        "                    self.optimizer[self.which_optimizer[self.current_iteration%l]].zero_grad()\n",
        "                    loss = self.get_loss(which_loss=self.which_optimizer[self.current_iteration%l])\n",
        "                    loss.backward()\n",
        "                    self.optimizer[self.which_optimizer[self.current_iteration%l]].step()\n",
        "\n",
        "        self.current_iteration += 1\n",
        "\n",
        "\n",
        "    # def get_negative_images(self, num_images):\n",
        "    #     # print('function get_negative_images')\n",
        "\n",
        "    #     self.loader_train_negative\n",
        "\n",
        "    #     return images, gt\n",
        "\n",
        "\n",
        "    def get_sample_embeddings(self, num_samples):\n",
        "        # print('function get_sample_embeddings')\n",
        "\n",
        "        sample_size = 10000\n",
        "\n",
        "        # Positive samples\n",
        "        N = self.settings['num_positive_samples']\n",
        "        randclasses = torch.randint(0, self.num_C + self.num_Cs_dash, (N,)).to(device)\n",
        "        pos_samples = [self.mu_sigma_distribution[str(int(c))].sample((1,)) for c in randclasses]\n",
        "        pos_gt = randclasses.clone().float()\n",
        "        pos_samples = torch.cat(pos_samples, dim=0).to(device)\n",
        "\n",
        "        return pos_samples, pos_gt\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        # print('function forward')\n",
        "\n",
        "        self.gt = Variable(torch.LongTensor(self.data['label'])).to(device).long()\n",
        "        img_source = Variable(self.data['img'][:, :3, :, :]).to(device).float()\n",
        "        self.gt_neg = Variable(torch.LongTensor(self.data_neg['label'])).to(device).long() + self.num_Cs\n",
        "        img_neg_source = Variable(self.data_neg['img'][:, :3, :, :]).to(device).float()\n",
        "\n",
        "        self.gt = torch.cat([self.gt, self.gt_neg], dim=0)\n",
        "\n",
        "        images = torch.cat([img_source, img_neg_source], dim=0) # Concatenate positive and negative images\n",
        "\n",
        "        self.features = {}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.features['M'] = self.network.M(images)\n",
        "        self.features['Fs'] = self.network.Fs(self.features['M'])\n",
        "        self.features['G'] = self.network.G(self.features['Fs'])\n",
        "        self.features['Cs'] = self.network.Cs(self.features['Fs'])\n",
        "        self.features['Cn'] = self.network.Cn(self.features['Fs'])\n",
        "\n",
        "        if self.current_iteration >= self.settings['val_after']:\n",
        "\n",
        "            if (self.mu_sigma_distribution == None) or (self.current_iteration % self.settings['val_after'] == 0):\n",
        "                self.recalculate_mu_sigma()\n",
        "\n",
        "            self.features['Fs_sample'], self.gt_sample = self.get_sample_embeddings((images.shape[0],)) # top half - positive, bottom half - negative\n",
        "            self.features['G_sample'] = self.network.G(self.features['Fs_sample'])\n",
        "            self.features['Fs_sample_recon'] = self.network.Fs(self.features['G_sample'])\n",
        "            self.features['Cs_sample'] = self.network.Cs(self.features['Fs_sample'])\n",
        "            self.features['Cn_sample'] = self.network.Cn(self.features['Fs_sample'])\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        # print('function train')\n",
        "\n",
        "        self.phase = 'train'\n",
        "\n",
        "        self.summary_dict = {}\n",
        "\n",
        "        try:\n",
        "            # self.data = self.dataloader_train.next()[1]\n",
        "            # self.data_neg = self.dataloader_train_negative.next()[1]\n",
        "\n",
        "            self.data = next(self.dataloader_train)[1]\n",
        "            self.data_neg = next(self.dataloader_train_negative)[1]\n",
        "\n",
        "            if self.data['img'].shape[0] < self.settings['batch_size']:\n",
        "                self.dataloader_train = enumerate(self.loader_train)\n",
        "                self.data = next(self.dataloader_train)[1]\n",
        "\n",
        "            if self.data_neg['img'].shape[0] < self.settings['batch_size']:\n",
        "                self.dataloader_train_negative = enumerate(self.loader_train_negative)\n",
        "                self.data_neg = next(self.dataloader_train_negative)[1]\n",
        "        except:\n",
        "            self.dataloader_train = enumerate(self.loader_train)\n",
        "            self.data = next(self.dataloader_train)[1]\n",
        "            self.dataloader_train_negative = enumerate(self.loader_train_negative)\n",
        "            self.data_neg = next(self.dataloader_train_negative)[1]\n",
        "\n",
        "        self.forward()\n",
        "        self.loss()\n",
        "\n",
        "        return self.summary_dict['acc/source_acc_pos_img']\n",
        "\n",
        "\n",
        "    def val(self):\n",
        "        # print('function val')\n",
        "\n",
        "        self.phase = 'val'\n",
        "\n",
        "        self.summary_dict = {}\n",
        "\n",
        "        self.forward()\n",
        "\n",
        "        self.loss()\n",
        "\n",
        "\n",
        "    def log_errors(self, phase, iteration=None):\n",
        "        # print('function log_errors')\n",
        "\n",
        "        print('log errors phase: ' + str(phase) + '\\n')\n",
        "        print(self.summary_dict.keys())\n",
        "\n",
        "        for x in list(sorted(self.summary_dict.keys())):\n",
        "            print(x + ' : ' + str(float(self.summary_dict[x])))\n",
        "\n",
        "            if phase == 'val':\n",
        "                self.val_writer.add_scalar(x, self.summary_dict[x], self.current_iteration)\n",
        "            elif phase == 'train':\n",
        "                self.train_writer.add_scalar(x, self.summary_dict[x], self.current_iteration)\n",
        "\n",
        "\n",
        "    def set_mode_val(self):\n",
        "        # print('function set_mode_val')\n",
        "\n",
        "        self.network.eval()\n",
        "        self.backward = False\n",
        "        for p in self.network.parameters():\n",
        "            p.requires_grad = False\n",
        "            p.volatile = True\n",
        "\n",
        "\n",
        "    def set_mode_train(self):\n",
        "        # print('function set_mode_train')\n",
        "\n",
        "        self.network.train()\n",
        "        self.backward = True\n",
        "        for p in self.network.parameters():\n",
        "            p.requires_grad = True\n",
        "            p.volatile = False\n",
        "\n",
        "        for comp in self.settings['to_train']:\n",
        "            if self.settings['to_train'][comp] == False:\n",
        "                self.network.components[comp].eval()\n",
        "                for p in self.network.components[comp].parameters():\n",
        "                    p.requires_grad = False\n",
        "                    p.volatile = True\n",
        "\n",
        "\n",
        "    def val_over_val_set(self):\n",
        "        # print('function val_over_val_set')\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            enough_iters = (self.current_iteration >= self.settings['val_after'])\n",
        "\n",
        "            self.summary_dict = {}\n",
        "\n",
        "            dataset_source_val = TemplateDataset(self.index_list_val_source, transform=transforms.Compose([transforms.ToTensor()]), random_choice=False)\n",
        "            dataloader_source = DataLoader(dataset_source_val, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "            # ----------------------\n",
        "            # Source validation Data\n",
        "            # ----------------------\n",
        "\n",
        "            print('\\nValidating on source validation data')\n",
        "\n",
        "            num_C = self.num_C\n",
        "            num_Cs_dash = self.num_Cs_dash\n",
        "            num_Ct_dash = self.num_Ct_dash\n",
        "            num_Cs = self.num_Cs\n",
        "            num_Ct = self.num_Ct\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                classes = list(range(num_C+num_Cs_dash))\n",
        "\n",
        "                avg_acc = {c:0 for c in classes}\n",
        "                avg_count = {c:0 for c in classes}\n",
        "\n",
        "                idx = -1\n",
        "\n",
        "                for data in tqdm(dataloader_source):\n",
        "                    idx += 1\n",
        "                    x = Variable(data['img'][:, :3, :, :]).to(self.settings['device']).float()\n",
        "                    labels_source = Variable(data['label']).to(self.settings['device'])\n",
        "\n",
        "                    M = self.network.components['M'](x)\n",
        "                    Fs = self.network.components['Fs'](M)\n",
        "                    G = self.network.components['G'](Fs)\n",
        "                    Cs = self.network.components['Cs'](Fs)\n",
        "                    Cn = self.network.components['Cn'](Fs)\n",
        "\n",
        "                    concat_outputs = torch.cat([Cs, Cn], dim=-1)\n",
        "                    concat_softmax = F.softmax(concat_outputs/self.settings['softmax_temperature'], dim=-1)\n",
        "\n",
        "                    max_act, pred = torch.max(concat_softmax, dim=-1)\n",
        "\n",
        "                    for c in classes:\n",
        "                        avg_acc[c] += (pred[labels_source==c] == labels_source[labels_source==c]).float().sum()\n",
        "                        avg_count[c] += pred[labels_source==c].shape[0]\n",
        "\n",
        "                # average accuracy\n",
        "                avg = 0\n",
        "                num_classes = num_C + num_Cs_dash\n",
        "                for c in classes:\n",
        "                    if avg_count[c] == 0:\n",
        "                        avg += 0\n",
        "                    else:\n",
        "                        avg += (float(avg_acc[c]) / float(avg_count[c]))\n",
        "                avg /= float(num_classes)\n",
        "                self.summary_dict['acc/source_avg'] = avg\n",
        "\n",
        "            return self.summary_dict\n",
        "\n",
        "\n",
        "    def get_mu_sigma(self): # For tracking variance of each class in the training dataset\n",
        "        # print('function get_mu_sigma')\n",
        "\n",
        "        dataset_train = TemplateDataset(self.index_list_train_source, transform=transforms.Compose([transforms.ToTensor()]), random_choice=False)\n",
        "        dataloader_source = DataLoader(dataset_train, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "        features_source = []\n",
        "        labels_source = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for data in tqdm(dataloader_source):\n",
        "                img = data['img'][:, :3, :, :].float().to(device)\n",
        "                lab = data['label'].float().to(device)\n",
        "\n",
        "                M = self.network.components['M'](img)\n",
        "                Fs = self.network.components['Fs'](M)\n",
        "\n",
        "                features_source.append(Fs)\n",
        "                labels_source.append(lab)\n",
        "\n",
        "            features_source = torch.cat(features_source, dim=0)\n",
        "            labels_source = torch.cat(labels_source, dim=0)\n",
        "\n",
        "        mu_sigma_data = get_mu_sig(features_source, labels_source, self.num_C + self.num_Cs_dash, only_return=False)\n",
        "\n",
        "        return mu_sigma_data"
      ],
      "metadata": {
        "id": "mYlmkakaBif8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "from matplotlib.figure import Figure\n",
        "from torch.utils.data import DataLoader\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "import subprocess\n",
        "import warnings\n",
        "from torchvision import transforms\n",
        "from tensorboardX import SummaryWriter\n",
        "import shutil\n",
        "\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "# ======================= SANITY CHECK ======================= #\n",
        "\n",
        "assert settings['running_supervised'], 'ERROR!! Config not set to run supervised trainer!!'\n",
        "print('######## SANITY CHECK ########')\n",
        "for key in sorted(settings.keys()):\n",
        "\tprint('{}: {}'.format(key, settings[key]))\n",
        "\n",
        "# ip = raw_input('continue? (y/n): ')\n",
        "# if ip.lower() == 'y' or ip.lower() == 'yes':\n",
        "# \tpass\n",
        "# else:\n",
        "# \tprint('Decided not to execute!')\n",
        "# \texit()\n",
        "\n",
        "# ==================== END OF SANITY CHECK ==================== #\n",
        "\n",
        "max_val_source_acc = -10000\n",
        "itt_delete = []\n",
        "\n",
        "def main():\n",
        "\n",
        "    print('\\n Setting up data sources ...')\n",
        "\n",
        "    # ====== DELETE PAST RUNS ======\n",
        "    #torch.cuda.set_device(settings['gpu'])\n",
        "    exp_name = settings['exp_name']\n",
        "    # subprocess.call([\"rm\", \"-rf\", os.path.join(settings['weights_path'],exp_name)])\n",
        "    # subprocess.call([\"mkdir\", os.path.join(settings['weights_path'],exp_name)])\n",
        "    # subprocess.call([\"rm\", \"-rf\", os.path.join(settings['summaries_path'],exp_name)])\n",
        "    # subprocess.call([\"mkdir\", os.path.join(settings['summaries_path'],exp_name)])\n",
        "    # subprocess.call([\"mkdir\", os.path.join(settings['summaries_path'],exp_name)+\"/logdir_train\"])\n",
        "    # subprocess.call([\"mkdir\", os.path.join(settings['summaries_path'],exp_name)+\"/logdir_val\"])\n",
        "\n",
        "    # Define the paths for weights and summaries\n",
        "    weights_path = settings['weights_path']\n",
        "    summaries_path = settings['summaries_path']\n",
        "\n",
        "    # Remove the existing directory for experiment's weights and summaries if they exist\n",
        "    weights_dir = os.path.join(weights_path)\n",
        "    if os.path.exists(weights_dir):\n",
        "        shutil.rmtree(weights_dir)  # Delete the directory and its contents\n",
        "    os.makedirs(weights_dir)  # Create a new empty directory for weights\n",
        "\n",
        "    summaries_dir = os.path.join(summaries_path)\n",
        "    if os.path.exists(summaries_dir):\n",
        "        shutil.rmtree(summaries_dir)  # Delete the directory and its contents\n",
        "    os.makedirs(summaries_dir)  # Create a new empty directory for summaries\n",
        "\n",
        "    # Create subdirectories for training and validation logs\n",
        "    train_log_dir = os.path.join(summaries_dir, \"logdir_train\")\n",
        "    val_log_dir = os.path.join(summaries_dir, \"logdir_val\")\n",
        "    os.makedirs(train_log_dir)  # Create training log directory\n",
        "    os.makedirs(val_log_dir)  # Create validation log directory\n",
        "\n",
        "    with open(os.path.join(settings['summaries_path'], 'txt'), 'w') as history_file:\n",
        "      print('saving in ' + os.path.join(settings['summaries_path'], 'txt'))\n",
        "      history_file.write('\\n===== x ===== x =====\\n')\n",
        "      for key in sorted(settings.keys()):\n",
        "        history_file.write('{}: {}\\n'.format(key, settings[key]))\n",
        "\n",
        "    # ====== DEFINE DATA SOURCES ======\n",
        "    index_list_path_train_source = os.path.join(settings['dataset_path'], 'source_images_index_list_train.npy')\n",
        "    index_list_path_val_source = os.path.join(settings['dataset_path'], 'source_images_index_list_val.npy')\n",
        "    index_lists = [index_list_path_train_source, index_list_path_val_source, None, None, None, None]\n",
        "\n",
        "    # ====== CREATE NETWORK ======\n",
        "    print('\\n Building network ...')\n",
        "    network = modnet(settings['num_C'], settings['num_Cs_dash'], settings['num_Ct_dash'], cnn=settings['cnn_to_use']).to(device)\n",
        "\n",
        "    # Load weights\n",
        "    if settings['load_weights']:\n",
        "      dict_to_load = torch.load(settings['load_weights_path'])\n",
        "      for component in dict_to_load:\n",
        "        network.components[component].load_state_dict(dict_to_load[component])\n",
        "\n",
        "    # ====== DEFINE OPTIMIZERS ======\n",
        "    print('\\n Setting up optimizers ...')\n",
        "    optimizer = {}\n",
        "\n",
        "    for key in settings['use_loss']:\n",
        "      if settings['use_loss'][key]:\n",
        "        to_train = []\n",
        "        for comp in settings['optimizer'][key]:\n",
        "          if settings['to_train'][comp]:\n",
        "            to_train.append({'params': network.components[comp].parameters(), 'lr':settings['lr']})#, 'momentum':0.1, 'nesterov':True, 'weight_decay':0.1})\n",
        "        optimizer[key] = optim.Adam(params = to_train)\n",
        "\n",
        "    def trainval(network, optimizer, exp_name, index_lists, settings):\n",
        "\n",
        "      global least_val_loss\n",
        "      global itt_delete\n",
        "\n",
        "      train_iter = settings['start_iter']\n",
        "\n",
        "      trainer_G = TrainerG(network, optimizer, exp_name, index_lists, settings)\n",
        "\n",
        "      train_acc_list = []\n",
        "\n",
        "      while True:\n",
        "\n",
        "        print (\"\\n----------- train_iter \" + str(train_iter) + ' -----------\\n')\n",
        "        trainer_G.set_mode_train()\n",
        "        acc_gen = trainer_G.train()\n",
        "        trainer_G.log_errors('train')\n",
        "        train_acc_list.append(acc_gen.data.cpu().numpy())\n",
        "\n",
        "        if train_iter%settings['val_after'] == 0:\n",
        "\n",
        "          print('validating')\n",
        "\n",
        "          trainer_G.set_mode_val()\n",
        "          min_val_flag=test(trainer_G)\n",
        "\n",
        "          print('min_val_flag', min_val_flag)\n",
        "\n",
        "          if(min_val_flag):\n",
        "            print (\"Saving - iteration\", train_iter)\n",
        "            dict_to_save = {component:network.components[component].cpu().state_dict() for component in network.components}\n",
        "\n",
        "            # dict_to_save = {\n",
        "            # \t'vgg': network.vgg.cpu().state_dict(),\n",
        "            # \t'feature_ext': network.feature_ext.cpu().state_dict(),\n",
        "            # \t'classifier_known': network.classifier_known.cpu().state_dict(),\n",
        "            # \t'dec_feature_ext': network.dec_feature_ext.cpu().state_dict(),\n",
        "            # \t'classifier_unknown': network.classifier_unknown.cpu().state_dict(),\n",
        "            # \t'classifier_open_set': network.classifier_open_set.cpu().state_dict()\n",
        "            # }\n",
        "\n",
        "            torch.save(dict_to_save, os.path.join(os.path.join(settings['weights_path'])+'/', 'best_' + str(train_iter) + '.pth'))\n",
        "            network.to(device)\n",
        "            itt_delete.append(train_iter)\n",
        "\n",
        "            # if(len(itt_delete)>100):\n",
        "            #   for k in itt_delete[:-100]:\n",
        "            #     subprocess.call(['rm', os.path.join(os.path.join(settings['weights_path'])+'/', 'best_' + str(k) + '.pth')])\n",
        "            #   itt_delete = itt_delete[-100:]\n",
        "\n",
        "            # Limit the number of saved checkpoints (keeping only the last 100)\n",
        "            if len(itt_delete) > 100:\n",
        "                for k in itt_delete[:-100]:\n",
        "                    checkpoint_path = os.path.join(settings['weights_path'], 'best_' + str(k) + '.pth')\n",
        "                    if os.path.exists(checkpoint_path):  # Check if the file exists before attempting to delete\n",
        "                        os.remove(checkpoint_path)  # Use os.remove to delete files in Colab\n",
        "\n",
        "                # Keep only the last 100 iterations\n",
        "                itt_delete = itt_delete[-100:]\n",
        "\n",
        "          if train_iter > settings['max_iter']:\n",
        "            break\n",
        "\n",
        "        train_iter += 1\n",
        "\n",
        "      print(\"Train Acc: \", train_acc_list[settings['max_iter']])\n",
        "\n",
        "\n",
        "    def test(trainer_G, iteration=None):\n",
        "      global max_val_source_acc\n",
        "\n",
        "      summary_dict=trainer_G.val_over_val_set()\n",
        "\n",
        "      print('source set validation')\n",
        "      print(summary_dict)\n",
        "      val_acc_source = summary_dict['acc/source_avg']\n",
        "      max_val_source_acc = max(val_acc_source,max_val_source_acc)\n",
        "      trainer_G.log_errors('val')\n",
        "\n",
        "      if( (max_val_source_acc==val_acc_source) ):\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "\n",
        "    # ====== CALL TRAINING AND VALIDATION PROCESS ======\n",
        "    trainval(network, optimizer, exp_name, index_lists, settings)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tmain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFjQBrUKGIzJ",
        "outputId": "56866369-e2cd-4342-a667-7e418b4331b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "######## SANITY CHECK ########\n",
            "C: ['back_pack', 'calculator', 'keyboard', 'monitor', 'mouse', 'mug', 'bike', 'laptop_computer', 'headphones', 'projector']\n",
            "Cs_dash: ['bike_helmet', 'bookcase', 'bottle', 'desk_chair', 'desk_lamp', 'desktop_computer', 'file_cabinet', 'letter_tray', 'mobile_phone', 'paper_notebook']\n",
            "Ct_dash: ['pen', 'phone', 'printer', 'punchers', 'ring_binder', 'ruler', 'scissors', 'speaker', 'stapler', 'tape_dispenser', 'trash_can']\n",
            "Fs_dims: 256\n",
            "batch_size: 64\n",
            "classification_weight: [1, 0.2]\n",
            "cnn_to_use: resnet50\n",
            "dataset_exp_name: usfda_office_31_DtoA\n",
            "dataset_path: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/index_lists\n",
            "device: cuda\n",
            "exp_name: usfda_office_31_DtoA\n",
            "gpu: 0\n",
            "load_exp_name: None\n",
            "load_weights: False\n",
            "losses_after_enough_iters: ['logprob', 'logsoftmax', 'pos_sample_recon']\n",
            "lr: 0.0001\n",
            "max_iter: 10\n",
            "negative_data_path: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/negative_images\n",
            "negative_mask_path: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/negative_masks\n",
            "num_C: 10\n",
            "num_Cs: 20\n",
            "num_Cs_dash: 10\n",
            "num_Ct: 21\n",
            "num_Ct_dash: 11\n",
            "num_negative_images: 64\n",
            "num_negative_samples: 32\n",
            "num_positive_images: 64\n",
            "num_positive_samples: 32\n",
            "online_augmentation_90_degrees: True\n",
            "optimizer: {'classification': ['M', 'Fs', 'Cs', 'Cn'], 'pos_img_recon': ['Fs', 'G'], 'pos_sample_recon': ['Fs', 'G'], 'logsoftmax': ['Fs']}\n",
            "running_supervised: True\n",
            "softmax_temperature: 1\n",
            "start_iter: 1\n",
            "summaries_path: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/summaries\n",
            "to_train: {'M': False, 'Fs': True, 'Ft': False, 'G': True, 'Cs': True, 'Cn': True}\n",
            "use_loss: {'classification': True, 'pos_img_recon': True, 'pos_sample_recon': True, 'logsoftmax': True}\n",
            "val_after: 5\n",
            "val_aug_imgs_mean_after_softmax: True\n",
            "val_aug_imgs_mean_before_softmax: False\n",
            "weights_path: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/weights\n",
            "\n",
            " Setting up data sources ...\n",
            "saving in /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/summaries/txt\n",
            "\n",
            " Building network ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 192MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Setting up optimizers ...\n",
            "\n",
            "optimizers: ['classification', 'logsoftmax', 'pos_img_recon', 'pos_sample_recon']\n",
            "\n",
            "\n",
            "----------- train_iter 1 -----------\n",
            "\n",
            "\n",
            "Applying loss logsoftmax\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/source_acc_pos_img', 'acc/source_acc_neg_img', 'acc/source_binary_acc_neg_img'])\n",
            "acc/source_acc_neg_img : 0.0\n",
            "acc/source_acc_pos_img : 0.0\n",
            "acc/source_binary_acc_neg_img : 0.96875\n",
            "\n",
            "----------- train_iter 2 -----------\n",
            "\n",
            "\n",
            "Applying loss pos_img_recon\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/source_acc_pos_img', 'acc/source_acc_neg_img', 'acc/source_binary_acc_neg_img', 'loss/pos_img_recon'])\n",
            "acc/source_acc_neg_img : 0.0\n",
            "acc/source_acc_pos_img : 0.0\n",
            "acc/source_binary_acc_neg_img : 0.890625\n",
            "loss/pos_img_recon : 875.0618896484375\n",
            "\n",
            "----------- train_iter 3 -----------\n",
            "\n",
            "\n",
            "Applying loss pos_sample_recon\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/source_acc_pos_img', 'acc/source_acc_neg_img', 'acc/source_binary_acc_neg_img'])\n",
            "acc/source_acc_neg_img : 0.0\n",
            "acc/source_acc_pos_img : 0.0\n",
            "acc/source_binary_acc_neg_img : 0.921875\n",
            "\n",
            "----------- train_iter 4 -----------\n",
            "\n",
            "\n",
            "Applying loss classification\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/source_acc_pos_img', 'acc/source_acc_neg_img', 'acc/source_binary_acc_neg_img', 'loss/classification'])\n",
            "acc/source_acc_neg_img : 0.0\n",
            "acc/source_acc_pos_img : 0.0\n",
            "acc/source_binary_acc_neg_img : 0.875\n",
            "loss/classification : 6.506418228149414\n",
            "\n",
            "----------- train_iter 5 -----------\n",
            "\n",
            "\n",
            "Recalculating mu sigma\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:05<00:00,  1.09s/it]\n",
            "100%|██████████| 20/20 [00:00<00:00, 39.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Applying loss logsoftmax\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/source_acc_pos_img', 'acc/source_acc_neg_img', 'acc/source_binary_acc_neg_img', 'acc/source_acc_pos_samples', 'loss/logsoftmax'])\n",
            "acc/source_acc_neg_img : 0.0\n",
            "acc/source_acc_pos_img : 0.234375\n",
            "acc/source_acc_pos_samples : 0.21875\n",
            "acc/source_binary_acc_neg_img : 0.875\n",
            "loss/logsoftmax : -9.238740921020508\n",
            "validating\n",
            "\n",
            "Validating on source validation data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:31<00:00, 31.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source set validation\n",
            "{'acc/source_avg': 0.08333333333333333}\n",
            "log errors phase: val\n",
            "\n",
            "dict_keys(['acc/source_avg'])\n",
            "acc/source_avg : 0.08333333333333333\n",
            "min_val_flag True\n",
            "Saving - iteration 5\n",
            "\n",
            "----------- train_iter 6 -----------\n",
            "\n",
            "\n",
            "Applying loss pos_img_recon\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/source_acc_pos_img', 'acc/source_acc_neg_img', 'acc/source_binary_acc_neg_img', 'acc/source_acc_pos_samples', 'loss/pos_img_recon'])\n",
            "acc/source_acc_neg_img : 0.0\n",
            "acc/source_acc_pos_img : 0.203125\n",
            "acc/source_acc_pos_samples : 0.21875\n",
            "acc/source_binary_acc_neg_img : 0.84375\n",
            "loss/pos_img_recon : 798.2153930664062\n",
            "\n",
            "----------- train_iter 7 -----------\n",
            "\n",
            "\n",
            "Applying loss pos_sample_recon\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/source_acc_pos_img', 'acc/source_acc_neg_img', 'acc/source_binary_acc_neg_img', 'acc/source_acc_pos_samples', 'loss/pos_sample_recon'])\n",
            "acc/source_acc_neg_img : 0.015625\n",
            "acc/source_acc_pos_img : 0.328125\n",
            "acc/source_acc_pos_samples : 0.21875\n",
            "acc/source_binary_acc_neg_img : 0.796875\n",
            "loss/pos_sample_recon : 299.22125244140625\n",
            "\n",
            "----------- train_iter 8 -----------\n",
            "\n",
            "\n",
            "Applying loss classification\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/source_acc_pos_img', 'acc/source_acc_neg_img', 'acc/source_binary_acc_neg_img', 'acc/source_acc_pos_samples', 'loss/classification'])\n",
            "acc/source_acc_neg_img : 0.0\n",
            "acc/source_acc_pos_img : 0.234375\n",
            "acc/source_acc_pos_samples : 0.34375\n",
            "acc/source_binary_acc_neg_img : 0.84375\n",
            "loss/classification : 5.665257930755615\n",
            "\n",
            "----------- train_iter 9 -----------\n",
            "\n",
            "\n",
            "Applying loss logsoftmax\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/source_acc_pos_img', 'acc/source_acc_neg_img', 'acc/source_binary_acc_neg_img', 'acc/source_acc_pos_samples', 'loss/logsoftmax'])\n",
            "acc/source_acc_neg_img : 0.0\n",
            "acc/source_acc_pos_img : 0.671875\n",
            "acc/source_acc_pos_samples : 0.25\n",
            "acc/source_binary_acc_neg_img : 0.59375\n",
            "loss/logsoftmax : -7.682830333709717\n",
            "\n",
            "----------- train_iter 10 -----------\n",
            "\n",
            "\n",
            "Recalculating mu sigma\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|██████████| 20/20 [00:00<00:00, 90.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Applying loss pos_img_recon\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/source_acc_pos_img', 'acc/source_acc_neg_img', 'acc/source_binary_acc_neg_img', 'acc/source_acc_pos_samples', 'loss/pos_img_recon'])\n",
            "acc/source_acc_neg_img : 0.0\n",
            "acc/source_acc_pos_img : 0.46875\n",
            "acc/source_acc_pos_samples : 0.5625\n",
            "acc/source_binary_acc_neg_img : 0.703125\n",
            "loss/pos_img_recon : 789.9507446289062\n",
            "validating\n",
            "\n",
            "Validating on source validation data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source set validation\n",
            "{'acc/source_avg': 0.39166666666666666}\n",
            "log errors phase: val\n",
            "\n",
            "dict_keys(['acc/source_avg'])\n",
            "acc/source_avg : 0.39166666666666666\n",
            "min_val_flag True\n",
            "Saving - iteration 10\n",
            "\n",
            "----------- train_iter 11 -----------\n",
            "\n",
            "\n",
            "Applying loss pos_sample_recon\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/source_acc_pos_img', 'acc/source_acc_neg_img', 'acc/source_binary_acc_neg_img', 'acc/source_acc_pos_samples', 'loss/pos_sample_recon'])\n",
            "acc/source_acc_neg_img : 0.015625\n",
            "acc/source_acc_pos_img : 0.609375\n",
            "acc/source_acc_pos_samples : 0.375\n",
            "acc/source_binary_acc_neg_img : 0.765625\n",
            "loss/pos_sample_recon : 161.9952392578125\n",
            "\n",
            "----------- train_iter 12 -----------\n",
            "\n",
            "\n",
            "Applying loss classification\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/source_acc_pos_img', 'acc/source_acc_neg_img', 'acc/source_binary_acc_neg_img', 'acc/source_acc_pos_samples', 'loss/classification'])\n",
            "acc/source_acc_neg_img : 0.0\n",
            "acc/source_acc_pos_img : 0.484375\n",
            "acc/source_acc_pos_samples : 0.59375\n",
            "acc/source_binary_acc_neg_img : 0.8125\n",
            "loss/classification : 5.2248616218566895\n",
            "\n",
            "----------- train_iter 13 -----------\n",
            "\n",
            "\n",
            "Applying loss logsoftmax\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/source_acc_pos_img', 'acc/source_acc_neg_img', 'acc/source_binary_acc_neg_img', 'acc/source_acc_pos_samples', 'loss/logsoftmax'])\n",
            "acc/source_acc_neg_img : 0.0\n",
            "acc/source_acc_pos_img : 0.75\n",
            "acc/source_acc_pos_samples : 0.625\n",
            "acc/source_binary_acc_neg_img : 0.546875\n",
            "loss/logsoftmax : -8.851177215576172\n",
            "\n",
            "----------- train_iter 14 -----------\n",
            "\n",
            "\n",
            "Applying loss pos_img_recon\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/source_acc_pos_img', 'acc/source_acc_neg_img', 'acc/source_binary_acc_neg_img', 'acc/source_acc_pos_samples', 'loss/pos_img_recon'])\n",
            "acc/source_acc_neg_img : 0.0\n",
            "acc/source_acc_pos_img : 0.671875\n",
            "acc/source_acc_pos_samples : 0.5\n",
            "acc/source_binary_acc_neg_img : 0.515625\n",
            "loss/pos_img_recon : 723.7185668945312\n",
            "\n",
            "----------- train_iter 15 -----------\n",
            "\n",
            "\n",
            "Recalculating mu sigma\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:06<00:00,  1.21s/it]\n",
            "100%|██████████| 20/20 [00:00<00:00, 85.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Applying loss pos_sample_recon\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/source_acc_pos_img', 'acc/source_acc_neg_img', 'acc/source_binary_acc_neg_img', 'acc/source_acc_pos_samples', 'loss/pos_sample_recon'])\n",
            "acc/source_acc_neg_img : 0.015625\n",
            "acc/source_acc_pos_img : 0.71875\n",
            "acc/source_acc_pos_samples : 0.6875\n",
            "acc/source_binary_acc_neg_img : 0.6875\n",
            "loss/pos_sample_recon : 114.83320617675781\n",
            "validating\n",
            "\n",
            "Validating on source validation data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source set validation\n",
            "{'acc/source_avg': 0.4666666666666666}\n",
            "log errors phase: val\n",
            "\n",
            "dict_keys(['acc/source_avg'])\n",
            "acc/source_avg : 0.4666666666666666\n",
            "min_val_flag True\n",
            "Saving - iteration 15\n",
            "0.609375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "from natsort import natsorted\n",
        "\n",
        "import matplotlib\n",
        "# matplotlib.use('Agg')\n",
        "\n",
        "import torch\n",
        "\n",
        "settings = {}\n",
        "\n",
        "settings['start_iter'] = 1\n",
        "settings['max_iter'] = 10\n",
        "\n",
        "settings['C'] = ['back_pack', 'calculator', 'keyboard', 'monitor', 'mouse', 'mug', 'bike', 'laptop_computer', 'headphones', 'projector']\n",
        "settings['Cs_dash'] = ['bike_helmet', 'bookcase', 'bottle', 'desk_chair', 'desk_lamp', 'desktop_computer', 'file_cabinet', 'letter_tray', 'mobile_phone', 'paper_notebook']\n",
        "settings['Ct_dash'] = ['pen', 'phone', 'printer', 'punchers', 'ring_binder', 'ruler', 'scissors', 'speaker', 'stapler', 'tape_dispenser', 'trash_can']\n",
        "\n",
        "settings['num_C'] = len(settings['C'])\n",
        "settings['num_Cs_dash'] = len(settings['Cs_dash'])\n",
        "settings['num_Ct_dash'] = len(settings['Ct_dash'])\n",
        "settings['num_Cs'] = settings['num_C'] + settings['num_Cs_dash']\n",
        "settings['num_Ct'] = settings['num_C'] + settings['num_Ct_dash']\n",
        "\n",
        "settings['val_after'] = 5\n",
        "settings['batch_size'] = 64\n",
        "\n",
        "settings['num_positive_samples'] = 32\n",
        "settings['num_negative_samples'] = 32\n",
        "settings['num_positive_images'] = settings['batch_size']\n",
        "settings['num_negative_images'] = settings['batch_size']\n",
        "\n",
        "settings['cnn_to_use'] = 'resnet50'\n",
        "settings['Fs_dims'] = 256\n",
        "settings['softmax_temperature'] = 1\n",
        "settings['online_augmentation_90_degrees'] = True # Used for online rotations in the data loader\n",
        "settings['val_aug_imgs_mean_before_softmax'] = False\n",
        "settings['val_aug_imgs_mean_after_softmax'] = True\n",
        "settings['separate_target_validation_set'] = True\n",
        "settings['target_train_val_split'] = 0.9\n",
        "\n",
        "xor = lambda a, b: ((a and not(b)) or (not(a) and b))\n",
        "assert xor(settings['val_aug_imgs_mean_after_softmax'], settings['val_aug_imgs_mean_before_softmax'])\n",
        "\n",
        "# For adapt\n",
        "settings['running_adapt'] = True\n",
        "settings['load_weights'] = True\n",
        "settings['load_exp_name'] = 'usfda_office_31_DtoA'\n",
        "settings['exp_name'] = 'usfda_office_31_DtoA_adapt'\n",
        "\n",
        "settings['optimizer'] = {\n",
        "\t'adaptation': ['Ft'],\n",
        "}\n",
        "\n",
        "settings['lambda'] = [1, 0.1]\n",
        "settings['weight_computation_method'] = 2\n",
        "settings['exponential_shift'] = 0\n",
        "\n",
        "settings['use_loss'] = {\n",
        "\t'adaptation': True,\n",
        "}\n",
        "\n",
        "settings['to_train'] = {\n",
        "\t'M': False, # -> only upto a certain conv layer. Needs to be frozen. We'll retrain the later layers.\n",
        "\t'Fs': False,\n",
        "\t'Ft': True,\n",
        "\t'G': False,\n",
        "\t'Cs': False,\n",
        "\t'Cn': False,\n",
        "}\n",
        "settings['lr'] = 1e-4 # 1e-3 default\n",
        "\n",
        "settings['lr'] = 1e-4\n",
        "settings['device'] = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    settings['gpu'] = torch.cuda.current_device()\n",
        "    torch.cuda.set_device(settings['gpu'])\n",
        "else:\n",
        "    settings['gpu'] = None  # Set to None when using CPU\n",
        "\n",
        "# Print device information for verification\n",
        "print(f\"Using device: {settings['device']}\")\n",
        "\n",
        "settings['weights_path'] = os.path.join(server_root_path, 'Office-31')\n",
        "settings['summaries_path'] = os.path.join(server_root_path, 'Office-31')\n",
        "\n",
        "# Loading Pretrained Weights\n",
        "if settings['load_weights']:\n",
        "    best_weights = natsorted(glob(os.path.join(settings['weights_path'], settings['load_exp_name'], 'weights', '*.pth')))[-1]\n",
        "    settings['load_weights_path'] = best_weights\n",
        "\n",
        "settings['dataset_exp_name'] = 'usfda_office_31_DtoA'\n",
        "\n",
        "# Dataset and Paths\n",
        "settings['dataset_path'] = os.path.join(server_root_path, 'Office-31', settings['dataset_exp_name'], 'index_lists')\n",
        "settings['negative_data_path'] = os.path.join(server_root_path, 'Office-31', settings['dataset_exp_name'], 'negative_images')\n",
        "settings['negative_mask_path'] = os.path.join(server_root_path, 'Office-31', settings['dataset_exp_name'], 'negative_masks')\n",
        "\n",
        "# Print settings to verify\n",
        "print(settings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZhEY1xMTNBW",
        "outputId": "a4807449-ac64-4541-d4d6-395b278c4b03"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "{'start_iter': 1, 'max_iter': 10, 'C': ['back_pack', 'calculator', 'keyboard', 'monitor', 'mouse', 'mug', 'bike', 'laptop_computer', 'headphones', 'projector'], 'Cs_dash': ['bike_helmet', 'bookcase', 'bottle', 'desk_chair', 'desk_lamp', 'desktop_computer', 'file_cabinet', 'letter_tray', 'mobile_phone', 'paper_notebook'], 'Ct_dash': ['pen', 'phone', 'printer', 'punchers', 'ring_binder', 'ruler', 'scissors', 'speaker', 'stapler', 'tape_dispenser', 'trash_can'], 'num_C': 10, 'num_Cs_dash': 10, 'num_Ct_dash': 11, 'num_Cs': 20, 'num_Ct': 21, 'val_after': 5, 'batch_size': 64, 'num_positive_samples': 32, 'num_negative_samples': 32, 'num_positive_images': 64, 'num_negative_images': 64, 'cnn_to_use': 'resnet50', 'Fs_dims': 256, 'softmax_temperature': 1, 'online_augmentation_90_degrees': True, 'val_aug_imgs_mean_before_softmax': False, 'val_aug_imgs_mean_after_softmax': True, 'separate_target_validation_set': True, 'target_train_val_split': 0.9, 'running_adapt': True, 'load_weights': True, 'load_exp_name': 'usfda_office_31_DtoA', 'exp_name': 'usfda_office_31_DtoA_adapt', 'optimizer': {'adaptation': ['Ft']}, 'lambda': [1, 0.1], 'weight_computation_method': 2, 'exponential_shift': 0, 'use_loss': {'adaptation': True}, 'to_train': {'M': False, 'Fs': False, 'Ft': True, 'G': False, 'Cs': False, 'Cn': False}, 'lr': 0.0001, 'device': device(type='cuda'), 'gpu': 0, 'weights_path': '/content/drive/MyDrive/Office-31', 'summaries_path': '/content/drive/MyDrive/Office-31', 'load_weights_path': '/content/drive/MyDrive/Office-31/usfda_office_31_DtoA/weights/best_15.pth', 'dataset_exp_name': 'usfda_office_31_DtoA', 'dataset_path': '/content/drive/MyDrive/Office-31/usfda_office_31_DtoA/index_lists', 'negative_data_path': '/content/drive/MyDrive/Office-31/usfda_office_31_DtoA/negative_images', 'negative_mask_path': '/content/drive/MyDrive/Office-31/usfda_office_31_DtoA/negative_masks'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8iWkPE_VhrS",
        "outputId": "5593a9f6-6ba6-4158-8c2c-09ed0ace777f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (4.25.5)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "from torchvision import transforms, utils\n",
        "import time\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "from matplotlib.figure import Figure\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "from skimage import io\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from tensorboardX import SummaryWriter\n",
        "import pdb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import norm\n",
        "from scipy.interpolate import make_interp_spline, BSpline\n",
        "from glob import glob\n",
        "\n",
        "class TrainerG():\n",
        "\n",
        "    def __init__(self, network, optimizer, exp_name, index_lists, settings):\n",
        "\n",
        "        # Set the network and optimizer\n",
        "        self.network = network\n",
        "        self.to_train = settings['to_train']\n",
        "\n",
        "        # Optimizers to use\n",
        "        self.optimizer = optimizer\n",
        "        self.which_optimizer = list(sorted(self.optimizer.keys()))\n",
        "        print('\\noptimizers: ' + str(self.which_optimizer) + '\\n')\n",
        "\n",
        "        # Save the settings\n",
        "        self.settings = settings\n",
        "\n",
        "        # Initialize the val and train writers\n",
        "        self.val_writer = SummaryWriter(os.path.join(server_root_path, 'Office-31', exp_name, 'summaries', 'logdir_val'))\n",
        "        self.train_writer = SummaryWriter(os.path.join(server_root_path, 'Office-31', exp_name, 'summaries', 'logdir_train'))\n",
        "\n",
        "        # Extract commonly used settings\n",
        "        self.batch_size = settings['batch_size']\n",
        "        self.current_iteration = settings['start_iter']\n",
        "\n",
        "        # Get the index lists\n",
        "        [_, _, index_list_path_train_target, index_list_path_val_target, _, index_list_path_aug_target] = index_lists\n",
        "        self.index_list_train_target = np.load(index_list_path_train_target)\n",
        "        self.index_list_val_target = np.load(index_list_path_val_target)\n",
        "        self.target_augmentation_dict = np.load(index_list_path_aug_target, allow_pickle=True).item() # dictionary is like {filename:[list, of, augmented, files, names]}\n",
        "\n",
        "        temp_aug_dict = {}\n",
        "        for k in self.target_augmentation_dict:\n",
        "            temp_aug_dict[os.path.join(server_root_path, k)] = [os.path.join(server_root_path, fn) for fn in self.target_augmentation_dict[k]]\n",
        "        self.target_augmentation_dict = temp_aug_dict\n",
        "\n",
        "        # Ensure augmented images from target validation set are removed\n",
        "        self.index_list_val_target = [s for s in self.index_list_val_target if s.split('/')[-1].split('_')[0] == 'category']\n",
        "\n",
        "        # Get number of classes\n",
        "        self.num_C = settings['num_C']\n",
        "        self.num_Cs_dash = settings['num_Cs_dash']\n",
        "        self.num_Ct_dash = settings['num_Ct_dash']\n",
        "        self.num_Cs = settings['num_Cs']\n",
        "        self.num_Ct = settings['num_Ct']\n",
        "\n",
        "        # Initialize data loaders\n",
        "        self.get_all_dataloaders()\n",
        "\n",
        "\n",
        "    def get_all_dataloaders(self):\n",
        "\n",
        "        dataset_train = TemplateDataset(self.index_list_train_target, transform=transforms.Compose([transforms.ToTensor()]), random_choice=False)\n",
        "        self.loader_train = DataLoader(dataset_train, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "        dataset_target_val = TemplateDataset(self.index_list_val_target, transform=transforms.Compose([transforms.ToTensor()]), random_choice=False)\n",
        "        self.loader_target_val = DataLoader(dataset_target_val, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "    def get_histogram(self, features, bin_size=0.01, normalize=True):\n",
        "\n",
        "        # print(features)\n",
        "        # print(len(features))\n",
        "\n",
        "        F = [int(float(f)/bin_size) for f in features]\n",
        "        D = {}\n",
        "        for f in F:\n",
        "            if f in D: D[f] += 1\n",
        "            else: D[f] = 1\n",
        "        x = np.array([float(k*bin_size) for k in sorted(D.keys())])\n",
        "        y = np.array([float(D[f]) for f in sorted(D.keys())])\n",
        "\n",
        "        if normalize:\n",
        "            y = y / np.sum(y)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "\n",
        "    def get_prefix(self):\n",
        "\n",
        "        return ''\n",
        "\n",
        "\n",
        "    def weight_computation_step(self, concat_softmax):\n",
        "\n",
        "        # Only exp max conf\n",
        "        if self.settings['weight_computation_method'] == 1:\n",
        "            num_Cs = self.num_C + self.num_Cs_dash\n",
        "            W, _ = torch.max(concat_softmax[:, :num_Cs], dim=-1)\n",
        "            W = torch.exp(self.settings['exponential_shift'] + W)\n",
        "            return W.squeeze(), (1-W).squeeze()\n",
        "\n",
        "        # Exp max conf normalized over batch\n",
        "        elif self.settings['weight_computation_method'] == 2:\n",
        "            num_Cs = self.num_C + self.num_Cs_dash\n",
        "            W, _ = torch.max(concat_softmax[:, :num_Cs], dim=-1)\n",
        "            W1 = torch.exp(self.settings['exponential_shift'] + W)\n",
        "            W1 = ( W1 - torch.min(W1) ) / ( torch.max(W1) - torch.min(W1) )\n",
        "            W2 = torch.exp(self.settings['exponential_shift'] + 1-W)\n",
        "            W2 = ( W2 - torch.min(W2) ) / ( torch.max(W2) - torch.min(W2) )\n",
        "            return W1.squeeze(), W2.squeeze()\n",
        "\n",
        "        # Sum conf in Cs / Cn\n",
        "        elif self.settings['weight_computation_method'] == 3:\n",
        "            num_Cs = self.num_C + self.num_Cs_dash\n",
        "            W = torch.sum(concat_softmax[:, :num_Cs], dim=-1)\n",
        "            return W.squeeze(), (1-W).squeeze()\n",
        "\n",
        "        # Sum conf in Cs / Cn normalized\n",
        "        elif self.settings['weight_computation_method'] == 4:\n",
        "            num_Cs = self.num_C + self.num_Cs_dash\n",
        "            W = torch.sum(concat_softmax[:, :num_Cs], dim=-1)\n",
        "            W1 = W / torch.max(W)\n",
        "            W2 = (1-W) / torch.max(1-W)\n",
        "            return W1.squeeze(), W2.squeeze()\n",
        "\n",
        "        # Sum conf in Cs / Cn exponential normalized\n",
        "        elif self.settings['weight_computation_method'] == 5:\n",
        "            num_Cs = self.num_C + self.num_Cs_dash\n",
        "            W = torch.sum(concat_softmax[:, :num_Cs], dim=-1)\n",
        "            W1 = torch.exp(self.settings['exponential_shift'] + W)\n",
        "            W1 = W / torch.max(W)\n",
        "            W2 = torch.exp(self.settings['exponential_shift'] + 1-W)\n",
        "            W2 = (1-W) / torch.max(1-W)\n",
        "            return W1.squeeze(), W2.squeeze()\n",
        "\n",
        "\n",
        "    def get_loss(self, which_loss):\n",
        "\n",
        "        if which_loss == 'adaptation':\n",
        "\n",
        "            num_Cs = self.num_C + self.num_Cs_dash\n",
        "\n",
        "            concat_outputs = torch.cat([self.features['Cs'], self.features['Cn']], dim=-1)\n",
        "            y_cap = F.softmax(concat_outputs/self.settings['softmax_temperature'], dim=-1)\n",
        "\n",
        "            w_concat_outputs = torch.cat([self.features['w_Cs'], self.features['w_Cn']], dim=-1)\n",
        "            w_y_cap = F.softmax(w_concat_outputs/self.settings['softmax_temperature'], dim=-1)\n",
        "            W1, W2 = self.weight_computation_step(w_y_cap)\n",
        "\n",
        "            # Detach W from the graph\n",
        "            W1 = torch.from_numpy(W1.cpu().detach().numpy()).to(device)\n",
        "\n",
        "            # Soft binary entropy way of pushing samples to the corresponding regions\n",
        "            y_cap_s = torch.sum(y_cap[:, :num_Cs], dim=-1)\n",
        "            y_cap_n = 1 - y_cap_s\n",
        "\n",
        "            Ld_1 = W1 * (-torch.log(y_cap_s)) + W2 * (-torch.log(y_cap_n))\n",
        "\n",
        "            # Soft categorical entropy way of pushing samples to the corresponding regions\n",
        "            y_tilde_s = F.softmax(self.features['Cs']/self.settings['softmax_temperature'], dim=-1)\n",
        "            y_tilde_n = F.softmax(self.features['Cn']/self.settings['softmax_temperature'], dim=-1)\n",
        "\n",
        "            H_s = - torch.sum(y_tilde_s * torch.log(y_tilde_s), dim=-1)\n",
        "            H_n = - torch.sum(y_tilde_n * torch.log(y_tilde_n), dim=-1)\n",
        "\n",
        "            Ld_2 = W1 * H_s + W2 * H_n\n",
        "\n",
        "            l1, l2 = self.settings['lambda']\n",
        "\n",
        "            loss_over_batch = Ld_1 * l1 + Ld_2 * l2\n",
        "\n",
        "            loss = torch.mean( loss_over_batch , dim=0 )\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError('Not implemented loss function ' + str(which_loss))\n",
        "\n",
        "        self.summary_dict['loss/' + str(which_loss)] = loss.data.cpu().numpy()\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def loss(self):\n",
        "\n",
        "        # ==================================\n",
        "        # ====== Accuracy over images ======\n",
        "        # ==================================\n",
        "\n",
        "        # Target Accuracy - all images\n",
        "        concat_outputs = torch.cat([self.features['Cs'], self.features['Cn']], dim=-1)\n",
        "        concat_softmax = F.softmax(concat_outputs/self.settings['softmax_temperature'], dim=-1)\n",
        "\n",
        "        pred = torch.argmax(concat_softmax, dim=-1)\n",
        "        pred[pred >= (self.num_C + self.num_Cs_dash)] = (self.num_C + self.num_Cs_dash)\n",
        "\n",
        "        target_acc = (pred.float() == self.gt.float()).float().mean()\n",
        "        self.summary_dict['acc/target_acc'] = target_acc\n",
        "\n",
        "        if self.phase == 'train':\n",
        "\n",
        "            # ====== BACKPROP LOSSES ======\n",
        "            enough_iters = (self.current_iteration >= self.settings['val_after'])\n",
        "            l = len(self.which_optimizer)\n",
        "            current_loss = self.which_optimizer[self.current_iteration%l]\n",
        "\n",
        "            if self.settings['use_loss'][current_loss] and self.backward:\n",
        "\n",
        "                print('\\nApplying loss ' + str(self.which_optimizer[self.current_iteration%l]))\n",
        "\n",
        "                self.optimizer[self.which_optimizer[self.current_iteration%l]].zero_grad()\n",
        "                loss = self.get_loss(which_loss=self.which_optimizer[self.current_iteration%l])\n",
        "                loss.backward()\n",
        "                self.optimizer[self.which_optimizer[self.current_iteration%l]].step()\n",
        "\n",
        "        self.current_iteration += 1\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "\n",
        "        self.gt = Variable(torch.LongTensor(self.data['label'])).to(device).float()\n",
        "        self.img_target = Variable(self.data['img'][:, :3, :, :]).to(device).float()\n",
        "        self.gt[self.gt >= self.num_C] = (self.num_C + self.num_Cs_dash) # Club all the target private classes into an unknown class\n",
        "\n",
        "        self.features = {}\n",
        "\n",
        "        # Target data\n",
        "        self.features['M'] = self.network.M(self.img_target)\n",
        "        self.features['Ft'] = self.network.Ft(self.features['M'])\n",
        "        self.features['Cs'] = self.network.Cs(self.features['Ft'])\n",
        "        self.features['Cn'] = self.network.Cn(self.features['Ft'])\n",
        "\n",
        "        # Passing target data through source cfier for getting the weight\n",
        "        with torch.no_grad():\n",
        "            self.features['w_Fs'] = self.network.Fs(self.features['M'])\n",
        "            self.features['w_Cs'] = self.network.Cs(self.features['w_Fs'])\n",
        "            self.features['w_Cn'] = self.network.Cn(self.features['w_Fs'])\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        self.phase = 'train'\n",
        "\n",
        "        self.summary_dict = {}\n",
        "\n",
        "        try:\n",
        "            self.data = next(self.dataloader_train)[1]\n",
        "            if self.data['img'].shape[0] < self.settings['batch_size']:\n",
        "                self.dataloader_train = enumerate(self.loader_train)\n",
        "                self.data = next(self.dataloader_train)[1]\n",
        "        except:\n",
        "            self.dataloader_train = enumerate(self.loader_train)\n",
        "            self.data = next(self.dataloader_train)[1]\n",
        "\n",
        "        self.forward()\n",
        "        self.loss()\n",
        "\n",
        "        return self.summary_dict['acc/target_acc']\n",
        "\n",
        "\n",
        "    def log_errors(self, phase, iteration=None):\n",
        "\n",
        "        print('log errors phase: ' + str(phase) + '\\n')\n",
        "        print(self.summary_dict.keys())\n",
        "\n",
        "        for x in self.summary_dict.keys():\n",
        "            print(x + ' : ' + str(float(self.summary_dict[x])))\n",
        "\n",
        "            if phase == 'val':\n",
        "                self.val_writer.add_scalar(self.get_prefix() + x, self.summary_dict[x], self.current_iteration)\n",
        "            elif phase == 'train':\n",
        "                self.train_writer.add_scalar(self.get_prefix() + x, self.summary_dict[x], self.current_iteration)\n",
        "\n",
        "\n",
        "    def set_mode_val(self):\n",
        "\n",
        "        self.network.eval()\n",
        "        self.backward = False\n",
        "        for p in self.network.parameters():\n",
        "            p.requires_grad = False\n",
        "            p.volatile = True\n",
        "\n",
        "\n",
        "    def set_mode_train(self):\n",
        "\n",
        "        self.network.train()\n",
        "        self.backward = True\n",
        "        for p in self.network.parameters():\n",
        "            p.requires_grad = True\n",
        "            p.volatile = False\n",
        "\n",
        "        for comp in self.settings['to_train']:\n",
        "            if self.settings['to_train'][comp] == False:\n",
        "                self.network.components[comp].eval()\n",
        "                for p in self.network.components[comp].parameters():\n",
        "                    p.requires_grad = False\n",
        "                    p.volatile = True\n",
        "\n",
        "\n",
        "    def val_over_val_set(self):\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            enough_iters = (self.current_iteration >= self.settings['val_after'])\n",
        "\n",
        "            self.summary_dict = {}\n",
        "\n",
        "            # --------------\n",
        "            # Target Dataset\n",
        "            # --------------\n",
        "\n",
        "            print('\\nValidating on target validation data')\n",
        "\n",
        "            dataset_target_val = TemplateDataset(self.index_list_val_target, transform=transforms.Compose([transforms.ToTensor()]), random_choice=False)\n",
        "            dataloader_target = DataLoader(dataset_target_val, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "            num_C = self.num_C\n",
        "            num_Cs_dash = self.num_Cs_dash\n",
        "            num_Ct_dash = self.num_Ct_dash\n",
        "            num_Cs = self.num_Cs\n",
        "            num_Ct = self.num_Ct\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                # Running calculations of accuracy\n",
        "\n",
        "                a_private_acc = 0\n",
        "                private_count = 0\n",
        "\n",
        "                classes = list(range(num_C))\n",
        "                classes.append(num_C+num_Cs_dash)\n",
        "\n",
        "                a_avg_count = {c:0 for c in classes}\n",
        "                a_avg_acc = {c:0 for c in classes}\n",
        "\n",
        "                idx = -1\n",
        "\n",
        "                for data in tqdm(dataloader_target):\n",
        "                    idx += 1\n",
        "                    x = Variable(data['img'][:, :3, :, :]).to(self.settings['device']).float()\n",
        "                    labels_target = Variable(data['label']).to(self.settings['device'])\n",
        "                    labels_target[labels_target>=num_C] = (self.num_C + self.num_Cs_dash) # The index corresponding to the Cn logit\n",
        "                    fnames = data['filename']\n",
        "\n",
        "                    M = self.network.components['M'](x)\n",
        "                    Ft = self.network.components['Ft'](M)\n",
        "                    G = self.network.components['G'](Ft)\n",
        "                    Cs = self.network.components['Cs'](Ft)\n",
        "                    Cn = self.network.components['Cn'](Ft)\n",
        "\n",
        "                    concat_outputs = torch.cat([Cs, Cn], dim=-1)\n",
        "                    concat_softmax = F.softmax(concat_outputs/self.settings['softmax_temperature'], dim=-1)\n",
        "\n",
        "                    max_act, pred = torch.max(concat_softmax, dim=-1)\n",
        "\n",
        "                    pred[pred>=(num_Cs)] = (self.num_C + self.num_Cs_dash) # Club all the negative classes into one\n",
        "\n",
        "                    private_count += pred[labels_target>=num_C].shape[0]\n",
        "\n",
        "                    for c in classes: # for all the shared and the unknown labels\n",
        "                        a_avg_count[c] += pred[labels_target==c].shape[0]\n",
        "\n",
        "                    a_pred = []\n",
        "                    for i, original_fname in enumerate(fnames):\n",
        "\n",
        "                        aug_imgs = []\n",
        "                        for fname in self.target_augmentation_dict[original_fname]:\n",
        "                            image = io.imread(fname)\n",
        "                            res = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
        "                            image = res(image)\n",
        "                            aug_imgs.append(image.view((1, 3, 224, 224)))\n",
        "                        aug_imgs = torch.cat(aug_imgs, dim=0).to(device)\n",
        "\n",
        "                        a_M = self.network.components['M'](aug_imgs)\n",
        "                        a_Ft = self.network.components['Ft'](a_M)\n",
        "                        a_Cs = self.network.components['Cs'](a_Ft)\n",
        "                        a_Cn = self.network.components['Cn'](a_Ft)\n",
        "\n",
        "                        a_concat_outputs = torch.cat([a_Cs, a_Cn], dim=-1)\n",
        "\n",
        "                        if self.settings['val_aug_imgs_mean_before_softmax']:\n",
        "                            a_concat_outputs = torch.mean(a_concat_outputs, dim=0)\n",
        "                        a_concat_softmax = F.softmax(a_concat_outputs/self.settings['softmax_temperature'], dim=-1)\n",
        "\n",
        "                        if self.settings['val_aug_imgs_mean_after_softmax']:\n",
        "                            a_concat_softmax = torch.mean(a_concat_softmax, dim=0)\n",
        "\n",
        "                        a_pred_temp = torch.LongTensor([int(torch.argmax(a_concat_softmax, dim=-1))]).to(device)\n",
        "                        if a_pred_temp >= num_Cs:\n",
        "                            a_pred_temp = torch.LongTensor([self.num_C + self.num_Cs_dash]).to(device)\n",
        "                        a_pred.append(a_pred_temp)\n",
        "\n",
        "                    a_pred = torch.cat(a_pred, dim=0).squeeze()\n",
        "\n",
        "                    a_private_acc += (a_pred[labels_target>=num_C] == labels_target[labels_target>=num_C]).float().sum()\n",
        "\n",
        "                    for c in classes:\n",
        "                        a_avg_acc[c] += (a_pred[labels_target==c] == labels_target[labels_target==c]).float().sum()\n",
        "\n",
        "                self.summary_dict['acc/target_a_private'] = float(a_private_acc) / float(private_count)\n",
        "\n",
        "                # average accuracy\n",
        "                a_avg = 0\n",
        "                num_classes = num_C + 1\n",
        "                classes = list(range(num_C))\n",
        "                classes.append(num_C+num_Cs_dash)\n",
        "                for c in classes:\n",
        "                    if a_avg_count[c] == 0:\n",
        "                        a_avg += 0\n",
        "                    else:\n",
        "                        a_avg += (float(a_avg_acc[c]) / float(a_avg_count[c]))\n",
        "                a_avg /= float(num_classes)\n",
        "                self.summary_dict['acc/target_a_avg'] = a_avg\n",
        "\n",
        "            return self.summary_dict['acc/target_a_avg']"
      ],
      "metadata": {
        "id": "IhoLUgNfVYC3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "from matplotlib.figure import Figure\n",
        "from torch.utils.data import DataLoader\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "import subprocess\n",
        "import warnings\n",
        "from torchvision import transforms\n",
        "from tensorboardX import SummaryWriter\n",
        "import shutil\n",
        "\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "# ======================= SANITY CHECK ======================= #\n",
        "\n",
        "assert settings['running_adapt'], 'ERROR!! Config not set to run adapt trainer!!'\n",
        "print('######## SANITY CHECK ########')\n",
        "for key in sorted(settings.keys()):\n",
        "\tprint('{}: {}'.format(key, settings[key]))\n",
        "\n",
        "# ip = raw_input('continue? (y/n): ')\n",
        "# if ip.lower() == 'y' or ip.lower() == 'yes':\n",
        "# \tpass\n",
        "# else:\n",
        "# \tprint('Decided not to execute!')\n",
        "# \texit()\n",
        "\n",
        "# ==================== END OF SANITY CHECK ==================== #\n",
        "\n",
        "max_val_acc = -10000\n",
        "itt_delete = []\n",
        "\n",
        "def main():\n",
        "\n",
        "    print('\\n Setting up data sources ...')\n",
        "\n",
        "    # ====== DELETE PAST RUNS ======\n",
        "    #torch.cuda.set_device(settings['gpu'])\n",
        "\n",
        "    exp_name = settings['exp_name']\n",
        "\n",
        "    # subprocess.call([\"rm\", \"-rf\", os.path.join(settings['weights_path'],exp_name)])\n",
        "    # subprocess.call([\"mkdir\", os.path.join(settings['weights_path'],exp_name)])\n",
        "    # subprocess.call([\"rm\", \"-rf\", os.path.join(settings['summaries_path'],exp_name)])\n",
        "    # subprocess.call([\"mkdir\", os.path.join(settings['summaries_path'],exp_name)])\n",
        "    # subprocess.call([\"mkdir\", os.path.join(settings['summaries_path'],exp_name)+\"/logdir_train\"])\n",
        "    # subprocess.call([\"mkdir\", os.path.join(settings['summaries_path'],exp_name)+\"/logdir_val\"])\n",
        "\n",
        "    # with open(os.path.join(settings['summaries_path'], settings['exp_name'], 'txt'), 'w') as history_file:\n",
        "    # \tprint('saving in ' + os.path.join(settings['summaries_path'], settings['exp_name'], 'txt'))\n",
        "    # \thistory_file.write('\\n===== x ===== x =====\\n')\n",
        "    # \tfor key in sorted(settings.keys()):\n",
        "    # \t\thistory_file.write('{}: {}\\n'.format(key, settings[key]))\n",
        "\n",
        "    # Define the paths for weights and summaries\n",
        "    weights_path = settings['weights_path']\n",
        "    summaries_path = settings['summaries_path']\n",
        "\n",
        "    # Remove the existing directory for experiment's weights and summaries if they exist\n",
        "    weights_dir = os.path.join(weights_path, exp_name)\n",
        "    if os.path.exists(weights_dir):\n",
        "        shutil.rmtree(weights_dir)  # Delete the directory and its contents\n",
        "    os.makedirs(weights_dir)  # Create a new empty directory for weights\n",
        "\n",
        "    summaries_dir = os.path.join(summaries_path, exp_name)\n",
        "    if os.path.exists(summaries_dir):\n",
        "        shutil.rmtree(summaries_dir)  # Delete the directory and its contents\n",
        "    os.makedirs(summaries_dir)  # Create a new empty directory for summaries\n",
        "\n",
        "    # Create subdirectories for training and validation logs\n",
        "    train_log_dir = os.path.join(summaries_dir, \"logdir_train\")\n",
        "    val_log_dir = os.path.join(summaries_dir, \"logdir_val\")\n",
        "    os.makedirs(train_log_dir)  # Create training log directory\n",
        "    os.makedirs(val_log_dir)  # Create validation log directory\n",
        "\n",
        "    with open(os.path.join(settings['summaries_path'], exp_name, 'txt'), 'w') as history_file:\n",
        "      print('saving in ' + os.path.join(settings['summaries_path'], exp_name, 'txt'))\n",
        "      history_file.write('\\n===== x ===== x =====\\n')\n",
        "      for key in sorted(settings.keys()):\n",
        "        history_file.write('{}: {}\\n'.format(key, settings[key]))\n",
        "\n",
        "    # ====== DEFINE DATA SOURCES ======\n",
        "    index_list_path_train_target = os.path.join(settings['dataset_path'], 'target_images_index_list_train.npy')\n",
        "    index_list_path_val_target = os.path.join(settings['dataset_path'], 'target_images_index_list_train.npy')\n",
        "    index_list_path_aug_target = os.path.join(settings['dataset_path'], 'target_images_aug_dict_train.npy')\n",
        "    index_lists = [None, None, index_list_path_train_target, index_list_path_val_target, None, index_list_path_aug_target]\n",
        "\n",
        "    # ====== CREATE NETWORK ======\n",
        "    print('\\n Building network ...')\n",
        "    network = modnet(settings['num_C'], settings['num_Cs_dash'], settings['num_Ct_dash'], cnn=settings['cnn_to_use']).to(device)\n",
        "\n",
        "    # Load weights\n",
        "    if settings['load_weights']:\n",
        "      dict_to_load = torch.load(settings['load_weights_path'])\n",
        "      for component in dict_to_load:\n",
        "\n",
        "        network.components[component].load_state_dict(dict_to_load[component])\n",
        "\n",
        "    # Initialize weights from source networks if we are loading from supervised experiment\n",
        "    if settings['load_exp_name'].split('_')[-1] != 'adapt':\n",
        "      # Initialize Ft from Fs\n",
        "      network.components['Ft'].load_state_dict(network.components['Fs'].state_dict())\n",
        "\n",
        "    # ====== DEFINE OPTIMIZERS ======\n",
        "    print('\\n Setting up optimizers ...')\n",
        "    optimizer = {}\n",
        "\n",
        "    for key in settings['use_loss']:\n",
        "      if settings['use_loss'][key]:\n",
        "        to_train = []\n",
        "        for comp in settings['optimizer'][key]:\n",
        "          if settings['to_train'][comp]:\n",
        "            to_train.append({'params': network.components[comp].parameters(), 'lr':settings['lr']})\n",
        "        optimizer[key] = optim.Adam(params = to_train)\n",
        "\n",
        "    # ====== CALL TRAINING AND VALIDATION PROCESS ======\n",
        "\n",
        "    def trainval(network, optimizer, exp_name, index_lists, settings):\n",
        "\n",
        "      global least_val_loss\n",
        "      global itt_delete\n",
        "\n",
        "      train_iter = settings['start_iter']\n",
        "\n",
        "      trainer_G = TrainerG(network, optimizer, exp_name, index_lists, settings)\n",
        "\n",
        "      train_acc_list = []\n",
        "\n",
        "      while True:\n",
        "\n",
        "        print (\"\\n----------- train_iter \" + str(train_iter) + ' -----------\\n')\n",
        "        trainer_G.set_mode_train()\n",
        "        acc_gen = trainer_G.train()\n",
        "        trainer_G.log_errors('train')\n",
        "        train_acc_list.append(acc_gen.data.cpu().numpy())\n",
        "\n",
        "        if train_iter%settings['val_after'] == 0:\n",
        "\n",
        "          print('validating')\n",
        "\n",
        "          trainer_G.set_mode_val()\n",
        "          min_val_flag=test(trainer_G)\n",
        "\n",
        "          print('min_val_flag', min_val_flag)\n",
        "\n",
        "          if(min_val_flag):\n",
        "            print (\"Saving - iteration\", train_iter)\n",
        "            dict_to_save = {component:network.components[component].cpu().state_dict() for component in network.components}\n",
        "            # dict_to_save = {\n",
        "            # \t'vgg': network.vgg.cpu().state_dict(),\n",
        "            # \t'feature_ext': network.feature_ext.cpu().state_dict(),\n",
        "            # \t'classifier_known': network.classifier_known.cpu().state_dict(),\n",
        "            # \t'dec_feature_ext': network.dec_feature_ext.cpu().state_dict(),\n",
        "            # \t'classifier_unknown': network.classifier_unknown.cpu().state_dict(),\n",
        "            # \t'classifier_open_set': network.classifier_open_set.cpu().state_dict()\n",
        "            # }\n",
        "            torch.save(dict_to_save, os.path.join(os.path.join(settings['weights_path'],exp_name)+'/', 'best_' + str(train_iter) + '.pth'))\n",
        "            network.to(device)\n",
        "            itt_delete.append(train_iter)\n",
        "\n",
        "            # Limit the number of saved checkpoints (keeping only the last 100)\n",
        "            if len(itt_delete) > 100:\n",
        "                for k in itt_delete[:-100]:\n",
        "                    checkpoint_path = os.path.join(settings['weights_path'], exp_name, 'best_' + str(k) + '.pth')\n",
        "                    if os.path.exists(checkpoint_path):  # Check if the file exists before attempting to delete\n",
        "                        os.remove(checkpoint_path)  # Use os.remove to delete files in Colab\n",
        "\n",
        "                # Keep only the last 100 iterations\n",
        "                itt_delete = itt_delete[-100:]\n",
        "\n",
        "          if train_iter > settings['max_iter']:\n",
        "            break\n",
        "\n",
        "        train_iter += 1\n",
        "\n",
        "      print(\"Train Acc: \", train_acc_list[settings['max_iter']])\n",
        "\n",
        "\n",
        "    def val_on_target_set(trainer_G_val_target, iteration):\n",
        "\n",
        "      global least_val_loss\n",
        "      global itt_delete\n",
        "\n",
        "      train_iter = 1\n",
        "\n",
        "      trainer_G = trainer_G_val_target\n",
        "\n",
        "      trainer_G.set_mode_val()\n",
        "      min_val_flag=test(trainer_G, target=True, iteration=iteration)\n",
        "\n",
        "\n",
        "    def test(trainer_G, target=False, iteration=None):\n",
        "      global max_val_acc\n",
        "      val_record=trainer_G.val_over_val_set()\n",
        "\n",
        "      print(val_record, max_val_acc)\n",
        "\n",
        "      print('source set validation')\n",
        "      print(val_record)\n",
        "      val_acc = val_record\n",
        "      # val_acc=val_record['source_classifier/accuracy_source_classifier_known_classes']\n",
        "      max_val_acc=max(val_acc,max_val_acc)\n",
        "      trainer_G.log_errors('val')\n",
        "\n",
        "      if(max_val_acc==val_acc):\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "\n",
        "    trainval(network, optimizer, exp_name, index_lists, settings)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tmain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfvmCbooXK8I",
        "outputId": "e38178fe-a28f-4d71-8175-162be3e2a99d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "######## SANITY CHECK ########\n",
            "C: ['back_pack', 'calculator', 'keyboard', 'monitor', 'mouse', 'mug', 'bike', 'laptop_computer', 'headphones', 'projector']\n",
            "Cs_dash: ['bike_helmet', 'bookcase', 'bottle', 'desk_chair', 'desk_lamp', 'desktop_computer', 'file_cabinet', 'letter_tray', 'mobile_phone', 'paper_notebook']\n",
            "Ct_dash: ['pen', 'phone', 'printer', 'punchers', 'ring_binder', 'ruler', 'scissors', 'speaker', 'stapler', 'tape_dispenser', 'trash_can']\n",
            "Fs_dims: 256\n",
            "batch_size: 64\n",
            "cnn_to_use: resnet50\n",
            "dataset_exp_name: usfda_office_31_DtoA\n",
            "dataset_path: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/index_lists\n",
            "device: cuda\n",
            "exp_name: usfda_office_31_DtoA_adapt\n",
            "exponential_shift: 0\n",
            "gpu: 0\n",
            "lambda: [1, 0.1]\n",
            "load_exp_name: usfda_office_31_DtoA\n",
            "load_weights: True\n",
            "load_weights_path: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/weights/best_15.pth\n",
            "lr: 0.0001\n",
            "max_iter: 10\n",
            "negative_data_path: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/negative_images\n",
            "negative_mask_path: /content/drive/MyDrive/Office-31/usfda_office_31_DtoA/negative_masks\n",
            "num_C: 10\n",
            "num_Cs: 20\n",
            "num_Cs_dash: 10\n",
            "num_Ct: 21\n",
            "num_Ct_dash: 11\n",
            "num_negative_images: 64\n",
            "num_negative_samples: 32\n",
            "num_positive_images: 64\n",
            "num_positive_samples: 32\n",
            "online_augmentation_90_degrees: True\n",
            "optimizer: {'adaptation': ['Ft']}\n",
            "running_adapt: True\n",
            "separate_target_validation_set: True\n",
            "softmax_temperature: 1\n",
            "start_iter: 1\n",
            "summaries_path: /content/drive/MyDrive/Office-31\n",
            "target_train_val_split: 0.9\n",
            "to_train: {'M': False, 'Fs': False, 'Ft': True, 'G': False, 'Cs': False, 'Cn': False}\n",
            "use_loss: {'adaptation': True}\n",
            "val_after: 5\n",
            "val_aug_imgs_mean_after_softmax: True\n",
            "val_aug_imgs_mean_before_softmax: False\n",
            "weight_computation_method: 2\n",
            "weights_path: /content/drive/MyDrive/Office-31\n",
            "\n",
            " Setting up data sources ...\n",
            "saving in /content/drive/MyDrive/Office-31/usfda_office_31_DtoA_adapt/txt\n",
            "\n",
            " Building network ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 148MB/s]\n",
            "<ipython-input-10-5fa1782f9a7a>:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  dict_to_load = torch.load(settings['load_weights_path'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Setting up optimizers ...\n",
            "\n",
            "optimizers: ['adaptation']\n",
            "\n",
            "\n",
            "----------- train_iter 1 -----------\n",
            "\n",
            "\n",
            "Applying loss adaptation\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/target_acc', 'loss/adaptation'])\n",
            "acc/target_acc : 0.5625\n",
            "loss/adaptation : 1.1391979455947876\n",
            "\n",
            "----------- train_iter 2 -----------\n",
            "\n",
            "\n",
            "Applying loss adaptation\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/target_acc', 'loss/adaptation'])\n",
            "acc/target_acc : 0.5\n",
            "loss/adaptation : 1.031150460243225\n",
            "\n",
            "----------- train_iter 3 -----------\n",
            "\n",
            "\n",
            "Applying loss adaptation\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/target_acc', 'loss/adaptation'])\n",
            "acc/target_acc : 0.625\n",
            "loss/adaptation : 0.9886326193809509\n",
            "\n",
            "----------- train_iter 4 -----------\n",
            "\n",
            "\n",
            "Applying loss adaptation\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/target_acc', 'loss/adaptation'])\n",
            "acc/target_acc : 0.625\n",
            "loss/adaptation : 0.9897570610046387\n",
            "\n",
            "----------- train_iter 5 -----------\n",
            "\n",
            "\n",
            "Applying loss adaptation\n",
            "log errors phase: train\n",
            "\n",
            "dict_keys(['acc/target_acc', 'loss/adaptation'])\n",
            "acc/target_acc : 0.53125\n",
            "loss/adaptation : 0.8802412748336792\n",
            "validating\n",
            "\n",
            "Validating on target validation data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/31 [03:41<1:50:55, 221.86s/it]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}